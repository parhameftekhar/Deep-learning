{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE-40959: Deep Learning\n",
    "## HW2 - MLP / Optimization Algorithms /  Batch Normalization / Dropout (Numpy)\n",
    "(18 points)\n",
    "\n",
    "### Deadline: 23 Esfand\n",
    "\n",
    "#### Name:\n",
    "#### Student No.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we're going to use the modules you implemented in Part1. We'll define a classification task and train a few models for it.\n",
    "\n",
    "The dataset we're going to use is called Cifar100. It contains 60000 rgb images (50000 for train and 10000 for test or validation) each with shape (32, 32, 3). Every image has a corresponding label which is a number in range 0 to 99, indicating a class of object. You can see the classes in the picture below.\n",
    "\n",
    "![title](images/cifar100.gif)\n",
    "\n",
    "We'll also ask you to compare the results of different models.\n",
    "\n",
    "** Keep in mind that the accuracy of Random Guess is 1 percent **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Imports\n",
    "Run this cell to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import *\n",
    "from adam import *\n",
    "from batch_norm import *\n",
    "from dense import *\n",
    "from dropout import *\n",
    "from module import *\n",
    "from optimizer import *\n",
    "from relu import *\n",
    "from sgd import *\n",
    "from sigmoid import *\n",
    "from softmax_crossentropy import *\n",
    "\n",
    "# we simply download cifar100 using tensorflow.keras library. this library is installed in google colab by default.\n",
    "from tensorflow.keras.datasets import cifar100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load the Data\n",
    "We download the dataset using `cifar100.load_data()` and store the results into (`X_train`, `y_train`), (`X_val`, `y_val`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 1828s 11us/step\n",
      "(50000, 32, 32, 3) (50000, 1)\n",
      "(10000, 32, 32, 3) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_val, y_val)= cifar100.load_data()\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using MLP for image classification, we need to flatten the images. Meaning reshaping the images to single dimension vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3072), (10000, 3072))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], -1)# flatten each image in X_train\n",
    "X_val = X_val.reshape(X_val.shape[0], -1)# flatten each image in X_val\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the labels to be scalers. We use `np.squeeze()` method to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000,), (10000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.squeeze(y_train)# squeeze y_train\n",
    "y_val = np.squeeze(y_val)# squeeze y_val\n",
    "y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.3. Normalization\n",
    " Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 0\n",
      "255 0\n"
     ]
    }
   ],
   "source": [
    "print(X_train.max(), X_train.min())\n",
    "print(X_val.max(), X_val.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, rgb values range from 0 to 255. We can devide every value by 255 to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n",
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train/255# devide X_train by 255\n",
    "X_val = X_val/255# devide X_val by 255\n",
    "print(X_train.max(), X_train.min())\n",
    "print(X_val.max(), X_val.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Plot Function\n",
    "This function get a history of training (loss, acc, val_loss, val_acc) as input and plots it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    losses, accs, val_losses, val_accs = history\n",
    "    x = [i for i in range(len(losses))]\n",
    "    \n",
    "    # plot for losses\n",
    "    plt.plot(x, losses, '-g', label='train')\n",
    "    plt.plot(x, val_losses, '-r', label='validation')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    # plot for accuracies\n",
    "    plt.plot(x, accs, '-g', label='train')\n",
    "    plt.plot(x, val_accs, '-r', label='validation')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Problem: Learning Rate (5 points)\n",
    "In this problem, We are going to see the effect of the `learning_rate` of `Adam` optimizer. We use the same model in the different sections of this problem. The only thing that is different, is the `learning_rate`.\n",
    "\n",
    "We use `beta1 = 0.9`, `beta2 = 0.999` and `epsilon = 1e-8` for `Adam`.\n",
    "The model contains a `Dense` layer with 100 neurones, followed by a `BatchNormalization` and a `ReLU` activation function. After that, we put another `Dense` layer with 100 neurones to output the probability of each class, followed by a `SoftmaxCrossentropy` module.\n",
    "\n",
    "We train each model for 10 epochs and `batch_size = 1024`. We also plot the history of training.\n",
    "\n",
    "**Note that you should assign a unique name for every module you define in your model.**\n",
    "\n",
    "You can use `model.add(Module)` to add a module to the model. The modules are added in order.\n",
    "It's also recommended to read the `model.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. `learning_rate = 0.001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: "
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'W'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ce7e4a36108b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dense2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_neurones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_neurones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_neurones\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSoftmaxCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/ALPR/deep/HW2/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_valid, y_valid, batch_size, epochs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/ALPR/deep/HW2/adam.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvt_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmt_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mm_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mv_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'W'"
     ]
    }
   ],
   "source": [
    "model = Model(Adam(learning_rate=1e-3, beta1=.9, beta2=.999))\n",
    "batch_size = 1024\n",
    "number_neurones = 100\n",
    "model.add(Dense('dense1', X_train.shape[1], number_neurones))\n",
    "model.optimizer.v['dense1'] = {'W': np.zeros((X_train.shape[1], number_neurones)), 'b': np.zeros((number_neurones,))}\n",
    "model.optimizer.m['dense1'] = {'W': np.zeros((X_train.shape[1], number_neurones)), 'b': np.zeros((number_neurones,))}\n",
    "model.add(BatchNormalization('batch1', number_neurones))\n",
    "model.optimizer.v['batch1'] = {'gamma': np.zeros((number_neurones,)), 'beta': np.zeros((number_neurones,))}\n",
    "model.optimizer.m['batch1'] = {'gamma': np.zeros((number_neurones,)), 'beta': np.zeros((number_neurones,))}\n",
    "model.add(ReLU('relu1'))\n",
    "model.add(Dense('dense2', number_neurones, number_neurones))\n",
    "model.optimizer.v['dense2'] = {'W': np.zeros((number_neurones, number_neurones)), 'b': np.zeros((number_neurones,))}\n",
    "model.optimizer.m['dense2'] = {'W': np.zeros((number_neurones, number_neurones)), 'b': np.zeros((number_neurones,))}\n",
    "model.add(SoftmaxCrossentropy('last'))\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=10)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. `learning_rate = 0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(Adam(learning_rate=1e-2, beta1=.9, beta2=.999))\n",
    "# add the layers to the model\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=10)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. `learning_rate = 0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(Adam(learning_rate=1e-1, beta1=.9, beta2=.999))\n",
    "# add the layers to the model\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=10)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. `learning_rate = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(Adam(learning_rate=1, beta1=.9, beta2=.999))\n",
    "# add the layers to the model\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=10)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Compare the Results\n",
    "** Compare the results of the models above. Which Learning Rate worked best? Why? (Feel free to write in Persian) **\n",
    "\n",
    "write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Problem: Optimizers & Batch Normalization (5 points)\n",
    "In this problem, We are going to try different optimizers with or without `BatchNormalization`. Except for optimizer and `BatchNormalization`, the other parts of the models in differenct sections are the same.\n",
    "\n",
    "We use `learning_rate = 0.1`, `beta1 = 0.9`, `beta2 = 0.999`, `epsilon = 1e-8` for `Adam` and `learning_rate = 0.1`, `momentum = 0.9` for SGD.\n",
    "\n",
    "\n",
    "The model contains a `Dense` layer with 100 neurones, followed by a `Sigmoid` activation function. After that, we put another `Dense` layer with 100 neurones to output the probability of each class, followed by a `SoftmaxCrossentropy` module.\n",
    "\n",
    "In some models, we add a `BatchNormalization` after the first `Dense` module.\n",
    "\n",
    "We train each model for 50 epochs and `batch_size = 1024`. We also plot the history of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. SGD & No BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(SGD(learning_rate=1e-1, momentum=.9))\n",
    "# add the layers to the model\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=50)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. SGD & BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(SGD(learning_rate=1e-1, momentum=.9))\n",
    "# add the layers to the model. don't forget to add a BatchNormalization module right after the first Dense module.\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=50)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Adam & BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(Adam(learning_rate=1e-1, beta1=.9, beta2=.999))\n",
    "# add the layers to the model. don't forget to add a BatchNormalization module right after the first Dense module.\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=50)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Compare the Results\n",
    "** Compare the results of the models above. **\n",
    "\n",
    "write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Problem: Regularization (8 points)\n",
    "Take another look at the results of 3.3. You should be able to see a significant gap between `acc` and `val_acc`. In this problem, We try to reduce the overfitting and make the model generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`L2 Regularization` restricts the scale of the weights of a `Dense` module and Reduces the complexity of the model.\n",
    "\n",
    "The `Dense` module you implemented, takes a `l2_coef` argument as input. In this problem, we try different values for `l2_coef`.\n",
    "\n",
    "The optimizers and the models' structure is the same as in 3.3.\n",
    "\n",
    "We train each model for 50 epochs and `batch_size = 1024`. We also plot the history of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. `l2_coef = 1e-2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(Adam(learning_rate=1e-1, beta1=.9, beta2=.999))\n",
    "# add the layers to the model. don't forget to set l2_coef for both Dense modules.\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=50)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. `l2_coef = 1e-3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(Adam(learning_rate=1e-1, beta1=.9, beta2=.999))\n",
    "# add the layers to the model. don't forget to set l2_coef for both Dense modules.\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=50)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. `l2_coef = 1e-4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(Adam(learning_rate=1e-1, beta1=.9, beta2=.999))\n",
    "# add the layers to the model. don't forget to set l2_coef for both Dense modules.\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=50)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4. `l2_coef = 1e-5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(Adam(learning_rate=1e-1, beta1=.9, beta2=.999))\n",
    "# add the layers to the model. don't forget to set l2_coef for both Dense modules.\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=50)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 Compare the Results\n",
    "** Compare the results of the models above. **\n",
    "\n",
    "write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Dropout\n",
    "A more common regularization method is `Dropout`. In this problem, we try different values for `keep_prob` for `Dropout` module and compare the results.\n",
    "\n",
    "The optimizers and the models' structure is the same as in 3.3. The only difference is we add a `Dropout` module right after `Sigmoid` activation.\n",
    "\n",
    "We train each model for 50 epochs and `batch_size = 1024`. We also plot the history of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. `keep_prob = 0.3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(Adam(learning_rate=1e-1, beta1=.9, beta2=.999))\n",
    "# add the layers to the model.\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=50)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. `keep_prob = 0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(Adam(learning_rate=1e-1, beta1=.9, beta2=.999))\n",
    "# add the layers to the model.\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=50)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. `keep_prob = 0.7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(Adam(learning_rate=1e-1, beta1=.9, beta2=.999))\n",
    "# add the layers to the model.\n",
    "history = model.fit(X_train, y_train, X_val, y_val, batch_size=1024, epochs=50)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4. Compare the Results\n",
    "** Compare the results of the models above. **\n",
    "\n",
    "write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Compare L2 Regularization and Dropout\n",
    "** Which one of the L2 Regularization and Dropout worked better? **\n",
    "\n",
    "write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
