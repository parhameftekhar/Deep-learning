{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CHDXBfEmWAJT"
   },
   "source": [
    "# CE-40959: Deep Learning\n",
    "## HW2 - MLP / Optimization Algorithms /  Batch Normalization / Dropout (Numpy)\n",
    "(24 points)\n",
    "### Deadline: 23 Esfand\n",
    "\n",
    "#### Name:\n",
    "#### Student No.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I90qRc4S1vgm"
   },
   "source": [
    "Please read the codes and instructions given to you carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8GOy2dW19v7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from model import *\n",
    "from adam import *\n",
    "from batch_norm import *\n",
    "from dense import *\n",
    "from dropout import *\n",
    "from module import *\n",
    "from optimizer import *\n",
    "from relu import *\n",
    "from sgd import *\n",
    "from sigmoid import *\n",
    "from softmax_crossentropy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSbUOPo11-q1"
   },
   "source": [
    "## 1. Modules\n",
    "In this problem you see some classes for typical modules used in neural networks. These modules include:\n",
    "\n",
    "1. **`ReLU`**: ReLU activation function\n",
    "\n",
    "2. **`Sigmoid`**: Sigmoid activation function\n",
    "\n",
    "3. **`SoftmaxCrossEntropy`**: A module which represents the softmax activation function followed by a cross entropy loss function.\n",
    "\n",
    "4. **`Dense`**: Fully connected layer which multiplies the input by a weight matrix and adds a bias term to it\n",
    "\n",
    "5. **`BatchNormalization`**: Batch Normalization layer\n",
    "\n",
    "6. **`Dropout`**: Dropout layer\n",
    "\n",
    "In this problem you have to implement `forward()` and `backward()` functions of the modules in the above list. Then your implemented codes will be tested here in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vKHSlGoN4aZi"
   },
   "source": [
    "### 1.1. Activation Functions\n",
    "Open files `relu.py` and `sigmoid.py`. There you can see **`ReLU`** and **`Sigmoid`** classes. \n",
    "Implement `forward()` and `backward()` functions of theses classes.\n",
    "\n",
    "Here are some tips that might help you:<br/>\n",
    "**1)** `forward()` function of the layers takes an input called `x` which is a numpy 2d-array with shape `(N, D)`. Implement the forward pass corresponding to considered function. Store the results of forward pass in `out`.<br/>\n",
    "**2)** `backward()` function of the layers takes a parameter called `dout`. `dout` is the gradient of loss w.r.t. the ouput of layer in the forward pass. Provided `dout`, you have to compute the gradient w.r.t. the inputs (i.e. `x`)  to the layer. Store the results in `dx`. <br/>\n",
    "**3)** For implementing backward pass, you may need some of the variables computed during forward pass; Save these variables in the `self.cache` attribute of the layer during forward pass and use them in the backward pass.<br/>\n",
    "**4)** Test your implementation with the following cells. You should see a small value as error between your funnctions outputs and `correct output`, `correct_dx` for each activation functions. We get error of orders 1e-8 and less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rP-EErtUyC9R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relu Test Cell:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ReLU' object has no attribute 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-96c9591d650c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mrelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Relu Test Cell:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m correct_output = np.array([[0.,         0. ,        1.08179168, 0.,         0.  ,       0.   ,     ],\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReLU' object has no attribute 'a'"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "#                             Relu Test                                   #\n",
    "###########################################################################\n",
    "np.random.seed(22)\n",
    "N = 5\n",
    "d = 6\n",
    "x = np.random.randn(N,d)\n",
    "relu = ReLU('test')\n",
    "print('Relu Test Cell:')\n",
    "output = relu.a()\n",
    "print(output)\n",
    "correct_output = np.array([[0.,         0. ,        1.08179168, 0.,         0.  ,       0.   ,     ],\n",
    "                  [0.9188215 , 0.  ,       0.62649346, 0. ,        0.02885512, 0.,        ],\n",
    "                  [0.58775221, 0.75231758, 0.   ,      1.05597241 ,0.74775027, 1.06467659],\n",
    "                  [1.52012959, 0.  ,       1.85998989, 0.  ,       0. ,        0.337325  ],\n",
    "                  [1.04672873 ,0.62914334, 0.36305909, 0.5557497,  0.,         0.02369477]])\n",
    "\n",
    "print('Relative error forward pass:', np.linalg.norm(output - correct_output))\n",
    "    \n",
    "dx = relu.backward(np.ones((N,d), dtype=np.float32))\n",
    "correct_dx = [[0., 0. ,1. ,0. ,0., 0.],\n",
    "              [1., 0. ,1., 0. ,1., 0.],\n",
    "              [1. ,1. ,0. ,1., 1., 1.],\n",
    "              [1. ,0. ,1., 0., 0. ,1.],\n",
    "              [1. ,1. ,1. ,1. ,0., 1.]]\n",
    "print('Relative error backward pass:', np.linalg.norm(dx - correct_dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8bxGWZGDanc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Test Cell:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-35ae47069579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                   [0.74014623, 0.65229519, 0.58978075, 0.63546853, 0.25189151, 0.50592342]]\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Relative error forward pass:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcorrect_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'list'"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "#                             sigmoid Test                                #\n",
    "###########################################################################\n",
    "np.random.seed(22)\n",
    "N=5\n",
    "d=6\n",
    "x = np.random.randn(N,d)\n",
    "sigmoid = Sigmoid('test')\n",
    "print('Sigmoid Test Cell:')    \n",
    "output = sigmoid.forward(x)\n",
    "correct_output = [[0.4770287,  0.18795539, 0.74683289, 0.44045266, 0.37962761, 0.26849495],\n",
    "                  [0.71480192, 0.24905997, 0.65169394, 0.36319727, 0.50721328, 0.44256287],\n",
    "                  [0.64284923, 0.67968348, 0.25759572, 0.74192012, 0.6786883,  0.74358323],\n",
    "                  [0.82055756, 0.18413151, 0.86529577, 0.16817555, 0.34387488, 0.58354059],\n",
    "                  [0.74014623, 0.65229519, 0.58978075, 0.63546853, 0.25189151, 0.50592342]]\n",
    "    \n",
    "print('Relative error forward pass:', np.linalg.norm(output - correct_output))\n",
    "    \n",
    "dx = sigmoid.backward(np.ones((N,d), dtype=np.float32))\n",
    "correct_dx = [[0.24947232, 0.15262816, 0.18907352, 0.24645411, 0.23551049, 0.19640541],\n",
    "              [0.20386014, 0.1870291,  0.22698895, 0.23128501, 0.24994797, 0.24670098],\n",
    "              [0.2295941,  0.21771385, 0.19124017, 0.19147466, 0.21807049, 0.19066721],\n",
    "              [0.14724285, 0.1502271,  0.116559,   0.13989254, 0.22562495, 0.24302097],\n",
    "              [0.19232979, 0.22680617, 0.24193942, 0.23164828, 0.18844218, 0.24996491]]\n",
    "    \n",
    "print('Relative error backward pass:', np.linalg.norm(dx - correct_dx))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VhU008dXEGab"
   },
   "source": [
    "### 1.2. Softmax activation with Cross Entropy loss\n",
    "\n",
    "You have to implement a numerically stable version of softmax in this problem.\n",
    "\n",
    "Open file `softmax_crossentropy.py`. There you see a class for Softmax activation with Cross Entropy loss used in neural networks.\n",
    " \n",
    "\n",
    "Implement `forward()` and `backward()` function of SoftmaxCrossentropy class corresponding to forward and backward pass of softmax activation followed by cross entropy loss. Test your implementation with the following functions. The order of outputs should be 1e-8 or smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnm4K_L-EbJh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax with Cross Entropy Test Cell:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-3b3e8a745995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcorrect_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.6883967462546619\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss relative error:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcorrect_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m correct_dx = [[ 0.00636809,  0.0106818,   0.01791759,  0.03005485,  0.05041383, -0.11543615],\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "#                  Softmax with Cross Entropy Test                        #\n",
    "###########################################################################\n",
    "np.random.seed(22)\n",
    "N=5\n",
    "d=6\n",
    "  \n",
    "x = np.linspace(1000, 1015, num=N*d).reshape(N,d)\n",
    "y = np.random.randint(0, d, (N, ))\n",
    "    \n",
    "softmax_ce = SoftmaxCrossentropy('test')\n",
    "print('Softmax with Cross Entropy Test Cell:')    \n",
    "loss, _ = softmax_ce.forward(x, y=y)\n",
    "dx = softmax_ce.backward()\n",
    "    \n",
    "correct_loss = 1.6883967462546619\n",
    "print('Loss relative error:', np.abs(loss - correct_loss))\n",
    "    \n",
    "correct_dx = [[ 0.00636809,  0.0106818,   0.01791759,  0.03005485,  0.05041383, -0.11543615],\n",
    "              [ 0.00636809,  0.0106818,   0.01791759,  0.03005485, -0.14958617,  0.08456385],\n",
    "              [ 0.00636809,  0.0106818,   0.01791759,  0.03005485, -0.14958617,  0.08456385],\n",
    "              [-0.19363191,  0.0106818,   0.01791759,  0.03005485,  0.05041383,  0.08456385],\n",
    "              [ 0.00636809,  0.0106818,   0.01791759,  0.03005485, -0.14958617,  0.08456385]]\n",
    "print('Gradient relative error:', np.linalg.norm(dx - correct_dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GY2kXfpbMwPO"
   },
   "source": [
    "### 1.3. Dense\n",
    "Open file `dense.py`. There you see a class for dense (or fully connected) layer used in neural networks.<br/> \n",
    "\n",
    "Implement `forward()` and `backward()` functions corresponding to forward and backward pass of a dense layer.<br/>\n",
    " \n",
    "We have implemented `__init__()` constructor for you. You can see `W`, `b`, `dW`, `db` variables in this class.\n",
    "\n",
    "In the forward pass, You have to output $y=XW+b$ in which $W$ and $b$ are parameters of the layer. In the backward pass you get $\\large{\\frac{\\partial{loss}}{\\partial{y}}}$ as `dout`; You have to compute $\\large{\\frac{\\partial{loss}}{\\partial{X}}}$, $\\large{\\frac{\\partial{loss}}{\\partial{W}}}$ and $\\large{\\frac{\\partial{loss}}{\\partial{b}}}$ and save them in `dx`, `dW`, and `db`. The output of the `backward()` function will be `dx`. \n",
    "\n",
    "Note that a dense layer with the shape of weights $(D, K)$ represents a layer in a MLP with $K$ neurons (or units). In other words, it takes a $D$-dimensional input and gives back a $K$-dimensional ouput.\n",
    "\n",
    "Test your implementation with the following functions. The order of ouputs should be 1e-8 or smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0FyQoJ8NiDi"
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#                         Dense Test                            #\n",
    "###########################################################################\n",
    "np.random.seed(22)\n",
    "D = 4\n",
    "K = 3\n",
    "N = 5\n",
    "x = np.random.randn(N,D)\n",
    "dense = Dense('test', D, K, l2_coef=1.)\n",
    "output = dense.forward(x)\n",
    "    \n",
    "correct_output = [[-0.51242952, -1.47921276, -2.32943713],\n",
    "                  [-1.17901283, -2.60908172,  0.54809823],\n",
    "                  [ 0.74600461, -2.24752841, -1.1013558 ],\n",
    "                  [ 0.75284837,  1.80111973, -2.27011589],\n",
    "                  [ 2.03171234, -3.05396933,  1.35213333]]\n",
    "    \n",
    "print('Relative error forward pass:', np.linalg.norm(output - correct_output))\n",
    "    \n",
    "dout = np.random.randn(N, K)\n",
    "dx = dense.backward(dout)\n",
    "    \n",
    "correct_dx = [[-0.25519113, -0.09724317,  0.280189,    0.87644613],\n",
    "              [ 1.20379991, -0.78816259, -1.27930227, -4.1952743 ],\n",
    "              [-0.77808532, -0.05005675, -3.14028536, -8.02818572],\n",
    "              [ 0.95446653, -1.90375857,  1.62080372,  3.57597736],\n",
    "              [ 2.86716776, -1.39892213,  0.31786772, -0.88234943]]\n",
    "print('Relative error dx:', np.linalg.norm(dx - correct_dx))\n",
    "    \n",
    "correct_dW = [[ 3.33629487, -4.43357113, -1.89100503],\n",
    "              [ 1.31103323,  2.17687036, -2.33906146],\n",
    "              [ 1.69538051, -0.89256682, -0.86018824],\n",
    "              [-0.87944724,  7.48073741, -7.0605863 ]]\n",
    "print('Relative error dw:', np.linalg.norm(dense.dW - correct_dW))\n",
    "    \n",
    "correct_db = [-1.02223284, -3.61915576, -0.16696389]\n",
    "print('Relative error db:', np.linalg.norm(dense.db - correct_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uqxyPhIXjXjt"
   },
   "source": [
    "### 1.4. Batch Normalization\n",
    "Open file `batch_norm.py`. There you see a class for Batch Normalization layer.<br/> \n",
    "\n",
    "Implement `forward()` and `backward()` functions corresponding to forward and backward pass of Batch Normalization layer.<br/>\n",
    " \n",
    "We have implemented `__init__()` constructor for you. Similar to `Dense` class, there are `gamma`, `beta`, `dgamma` and `dbeta` variables in this class.\n",
    "Test your implementations with the following cell. The order of our error is 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CAupHB8PkZGm"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-e9e543227420>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     [ 2.86418245, -1.48792145, -0.35683912,  0.13800476]]\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Realtive error normalized x:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_normal\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcorrect_x_normal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'list'"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "#            Batch-Normalization forward pass Test (Train)                #\n",
    "###########################################################################\n",
    "np.random.seed(22)\n",
    "N = 5\n",
    "D = 4\n",
    "\n",
    "x = np.random.randn(N,D)\n",
    "    \n",
    "batchnorm = BatchNormalization('test', D)\n",
    "batchnorm.train()\n",
    "\n",
    "x_normal = batchnorm.forward(x)\n",
    "    \n",
    "correct_x_normal = [[-0.36934148,  2.60787468, -0.04804626,  0.61796809],\n",
    "                    [-1.90650387,  1.86085579,  0.06676021,  0.28590773],\n",
    "                    [ 2.3972458,   1.14675911,  0.69370866,  0.62125601],\n",
    "                    [ 2.24806073, -0.98185142,  1.45971198,  1.11561189],\n",
    "                    [ 2.86418245, -1.48792145, -0.35683912,  0.13800476]]\n",
    "\n",
    "print('Realtive error normalized x:', np.linalg.norm(x_normal - correct_x_normal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEz_-F7FlRnS"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-0baab70e9778>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                   \u001b[0;34m[\u001b[0m \u001b[0;36m0.82878218\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.31966212\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m1.95469806\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                   [ 0.58280269,  0.64530383,  1.08884352]]\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Relative error:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_norm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcorrect_a_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'list'"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "#            Batch-Normalization forward pass Test (Test)                 #\n",
    "###########################################################################\n",
    "np.random.seed(22)\n",
    "\n",
    "N, D1, D2 = 5, 4, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "\n",
    "batchnorm = BatchNormalization('test', D2)\n",
    "batchnorm.train()\n",
    "    \n",
    "for t in range(50):\n",
    "    X = np.random.randn(N, D1)\n",
    "    a = np.maximum(0, np.matmul(X, W1))\n",
    "    batchnorm.forward(a)\n",
    "\n",
    "batchnorm.test()\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, np.matmul(X, W1))\n",
    "a_norm = batchnorm.forward(a)\n",
    "    \n",
    "correct_a_norm = [[ 2.67793539,  0.11029149, -2.26691758],\n",
    "                  [ 0.58280269,  1.69500735,  1.48454034],\n",
    "                  [ 0.79307162,  0.11029149,  0.94098477],\n",
    "                  [ 0.82878218,  0.31966212,  1.95469806],\n",
    "                  [ 0.58280269,  0.64530383,  1.08884352]]\n",
    "print('Relative error:', np.linalg.norm(a_norm - correct_a_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hUjSwHs3murz"
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#              Batch-Normalization backward pass Test                     #\n",
    "###########################################################################\n",
    "np.random.seed(22)\n",
    "N, D = 5, 4\n",
    "x = 2 * np.random.randn(N, D) + 10\n",
    "\n",
    "batchnorm = BatchNormalization('test', D)\n",
    "batchnorm.train()\n",
    "        \n",
    "dout = np.random.randn(N, D)\n",
    "out = batchnorm.forward(x)\n",
    "dx = batchnorm.backward(dout)\n",
    "    \n",
    "correct_dx = [[-1.31204675, -0.02199192, -0.94266767, -0.44927898],\n",
    "              [ 0.68352166, -0.01100818,  0.23785382,  0.09507173],\n",
    "              [ 0.34697892,  0.02983054, -0.11237967, -0.1803218 ],\n",
    "              [ 1.33026886,  0.09552155,  0.16976962,  0.29533059],\n",
    "              [-1.04872269, -0.09235199,  0.6474239,   0.23919846]]\n",
    "    \n",
    "print('Relative error dx:', np.linalg.norm(dx - correct_dx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kc1Js4hxqeL4"
   },
   "source": [
    "### 1.5. Dropout\n",
    "Open file `dropout.py`. There you see a class for Drop-out.<br/> \n",
    "\n",
    "Implement `forward()` and `backward()` functions of `Dropout` class corresponding to forward and backward pass of `inverted dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRFlePxZrIB6"
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#                           Drop-out Test                                 #\n",
    "###########################################################################\n",
    "np.random.seed(42)\n",
    "N = 5\n",
    "D = 5\n",
    "x = np.random.randint(0, 6, (N,D)).astype(np.float32)\n",
    "\n",
    "p = 0.5\n",
    "dropout = Dropout('test', p)\n",
    "dropout.train()\n",
    "out = dropout.forward(x)\n",
    "\n",
    "correct_out =  [[6., 8., 0., 8., 8.],\n",
    "                [0., 4., 4., 0., 8.],\n",
    "                [0., 4., 0., 0., 2.],\n",
    "                [6., 0., 0., 2., 6.],\n",
    "                [8., 0., 0., 0., 0.]]\n",
    "\n",
    "dout = np.ones((N,D))\n",
    "dx = dropout.backward(dout)\n",
    "\n",
    "correct_dx =  [[2., 2., 0., 2., 2.],\n",
    "               [0., 2., 2., 0., 2.],\n",
    "               [0., 2., 0., 0., 2.],\n",
    "               [2., 0., 0., 2., 2.],\n",
    "               [2., 2., 0., 0., 0.]]\n",
    "\n",
    "x = np.random.randint(0, 6, (N,D)).astype(np.float32)\n",
    "p = 0.5\n",
    "dropout = Dropout('test', p)\n",
    "dropout.test()\n",
    "out_test = dropout.forward(x)\n",
    "correct_out_test= [[2., 5., 0., 3., 1.],\n",
    "                   [3., 1., 5., 5., 5.],\n",
    "                   [1., 3., 5., 4., 1.],\n",
    "                   [1., 3., 1., 1., 5.],\n",
    "                   [3., 5., 5., 3., 0.]]\n",
    "print('Relative error Train forward pass output:', np.linalg.norm(out - correct_out))\n",
    "print('Relative error Train forward pass dx:', np.linalg.norm(dx - correct_dx))\n",
    "print('Relative error Test forward pass output:', np.linalg.norm(out_test - correct_out_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbIbV_oku_fq"
   },
   "source": [
    "## 2. Optimization Algorithms\n",
    "In this problem you see some function for different optimization algorithm used in neural networks. These algorithms are **`SGD`** and **`ADAM`**.\n",
    "\n",
    "Open files `sgd.py` and `adam.py`.<br/> \n",
    "you have to implement `SGD` and `Adam` classes.\n",
    "\n",
    "Test your implementation with the following cells. The order of errors should be 1e-8 or smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMwdZUmqRMQO"
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#                           SGD+Momentum Test                             #\n",
    "###########################################################################\n",
    "N, D = 5, 4\n",
    "np.random.seed(22)\n",
    "dense = Dense('test', N, D, l2_coef=1.)\n",
    "dense.dW = np.random.randn(N, D)\n",
    "dense.db = np.random.randn(D,)\n",
    "\n",
    "sgd = SGD(1e-2)\n",
    "sgd.velocities['test'] = {'W': np.random.randn(N, D), 'b': np.zeros_like(dense.b)}\n",
    "\n",
    "sgd.update(dense)\n",
    "\n",
    "correct_W = [[-1.04578269, -2.09413292,  1.74896632,  0.23833633],\n",
    "             [-1.13969324, -0.50236489,  1.28289011, -1.20095538],\n",
    "             [-0.2181534,  -0.12424752, -1.3418189,   0.13508095],\n",
    "             [ 0.59827594, -0.35371713, -2.00549095,  3.3314237 ],\n",
    "             [-1.09332467,  1.15610425,  1.24786695, -1.06838115],]\n",
    "correct_b = [ 1.86566377, -1.59381353, -0.62684131,  0.33332912]\n",
    "\n",
    "print('W Relative error:', np.linalg.norm(correct_W - dense.W))\n",
    "print('b Relative error: ', np.linalg.norm(correct_b - dense.b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xAf6hTyvRMQS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W error:  0.00674782599961274\n",
      "v error:  0.001817049601582242\n",
      "m error:  0.40710790137001135\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "#                                ADAM Test                                #\n",
    "###########################################################################\n",
    "N, D = 5, 4\n",
    "dense = Dense('test', N, D, l2_coef=1.)\n",
    "dense.W = np.linspace(-1, 1, N * D).reshape(N, D)\n",
    "dense.dW = np.linspace(-1, 1, N * D).reshape(N, D)\n",
    "dense.db = np.zeros(D,)\n",
    "adam = Adam(1e-2)\n",
    "\n",
    "m = np.linspace(0.6, 0.9,  N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5,  N * D).reshape(N, D)\n",
    "\n",
    "adam.m['test'] = {'W': m, 'b': np.zeros(D,)}\n",
    "adam.v['test'] = {'W': v, 'b': np.zeros(D,)}\n",
    "adam.iteration_number = 6\n",
    "adam.update(dense)\n",
    "\n",
    "next_param = dense.W\n",
    "\n",
    "correct_next_param = [[-1.00086812, -0.89566086, -0.79045452, -0.68524913],\n",
    "                      [-0.58004471, -0.47484131, -0.36963895, -0.26443768],\n",
    "                      [-0.15923753, -0.05403855,  0.05115923,  0.15635575],\n",
    "                      [ 0.26155096,  0.36674482,  0.47193728,  0.57712826],\n",
    "                      [ 0.68231771,  0.78750557,  0.89269177,  0.99787623]]\n",
    "correct_v = [[0.7003,     0.68958476, 0.67889169, 0.66822078],\n",
    "             [0.65757202, 0.64694543, 0.636341,   0.62575873],\n",
    "             [0.61519861, 0.60466066, 0.59414488, 0.58365125],\n",
    "             [0.57317978, 0.56273047, 0.55230332, 0.54189834],\n",
    "             [0.53151551, 0.52115485, 0.51081634, 0.5005    ]]\n",
    "correct_m = [[0.44,       0.46473684, 0.48947368, 0.51421053],\n",
    "             [0.53894737, 0.56368421, 0.58842105, 0.61315789],\n",
    "             [0.63789474, 0.66263158, 0.68736842, 0.71210526],\n",
    "             [0.73684211, 0.76157895, 0.78631579, 0.81105263],\n",
    "             [0.83578947, 0.86052632, 0.88526316, 0.91      ]]\n",
    "\n",
    "\n",
    "print('W error: ', np.linalg.norm(correct_next_param - next_param))\n",
    "print('v error: ', np.linalg.norm(correct_v - adam.v['test']['W']))\n",
    "print('m error: ', np.linalg.norm(correct_m - adam.m['test']['W']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlAt_iKCRMQW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Part1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
