{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12a8927bd3374e3e9ea651ad814e9c9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6e0ac91feb0a41f5acad30c50dab6408",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b891367997a242b9ab8de79aa4375b7f",
              "IPY_MODEL_7f64eaff0ae34e6ea8c69184c1cff624"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "6e0ac91feb0a41f5acad30c50dab6408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "b891367997a242b9ab8de79aa4375b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0aa7f053df754a859b32563f250b407f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9c734035dec1441c8d7d7de9090cd998"
          },
          "model_module_version": "1.5.0"
        },
        "7f64eaff0ae34e6ea8c69184c1cff624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_93c2c96f64724fd29c67302cb82f7e73",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:31&lt;00:00, 16626734.26it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7584e252572444b2a15b497629c53c2f"
          },
          "model_module_version": "1.5.0"
        },
        "0aa7f053df754a859b32563f250b407f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "9c734035dec1441c8d7d7de9090cd998": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "93c2c96f64724fd29c67302cb82f7e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "7584e252572444b2a15b497629c53c2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVVeVEgumZLg"
      },
      "source": [
        "# CE-40959: Deep Learning\n",
        "## HW2 - CIFAR-10 Classification (Pytorch)\n",
        "\n",
        "(18 points)\n",
        "\n",
        "### Deadline: 23 Esfand\n",
        "\n",
        "#### Name:\n",
        "#### Student No.:\n",
        "\n",
        "\n",
        "Please review `Pytorch Tutorial` notebook (materials of the TA classes) before coming to this notebook and you can use `pytorch.org` to learn how to use PyTorch classes and commands.\n",
        "\n",
        "In this part you have to implement MLP for Classification of CIFAR-10 dataset. \n",
        "\n",
        "PyTorch provides the elegantly designed modules and classes `torch.nn`, `torch.optim` , `Dataset` , and `DataLoader` to help you create and train neural networks. In this homework you use them for your implementations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5alOnjtlGfy"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGj-LMuWmx2q"
      },
      "source": [
        "#### 3.1. Load Data:\n",
        "\n",
        "Complete the followed cell for data loading. \n",
        "In this cell you have to normalize, split and shuffle data for learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgv51um_lJiL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "12a8927bd3374e3e9ea651ad814e9c9e",
            "6e0ac91feb0a41f5acad30c50dab6408",
            "b891367997a242b9ab8de79aa4375b7f",
            "7f64eaff0ae34e6ea8c69184c1cff624",
            "0aa7f053df754a859b32563f250b407f",
            "9c734035dec1441c8d7d7de9090cd998",
            "93c2c96f64724fd29c67302cb82f7e73",
            "7584e252572444b2a15b497629c53c2f"
          ]
        },
        "outputId": "6df99e1e-c35b-4a00-8ea8-291b1600360f"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "transform = transforms.Compose(\n",
        "            [#transforms.Resize((3*32*32, 1)),\n",
        "             transforms.ToTensor(),\n",
        "             transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                  std=[0.5, 0.5, 0.5]),\n",
        "             ])\n",
        "# koja save mishe in dataset        \n",
        "train_valid_set = torchvision.datasets.CIFAR10(root='/content/gdrive/data', train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='/content/gdrive/data', train=False, download=True, transform=transform)\n",
        "train_size = 0.9\n",
        "num_train_valid = len(train_valid_set)\n",
        "split = int(np.floor(train_size * num_train_valid))\n",
        "train_set, valid_set = torch.utils.data.random_split(train_valid_set,(split, num_train_valid-split))\n",
        "\n",
        "\n",
        "#num_train = len(train_valid_set)\n",
        "#indices = list(range(num_train))\n",
        "\n",
        "#np.random.shuffle(indices)\n",
        "#train_idx, valid_idx = indices[split:], indices[:split]\n",
        "#train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
        "#valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(valid_idx)\n",
        "\n",
        "\n",
        "#train_set, valid_set = train_valid_set[split:], train_valid_set[:split] \n",
        "    \n",
        "batch_size_train = 100\n",
        "batch_size_test = 100\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle = True,\n",
        "                                           num_workers=2)\n",
        "validationloader = torch.utils.data.DataLoader(valid_set, batch_size=100, shuffle = True,\n",
        "                                           num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle = True,\n",
        "                                           num_workers=2)\n",
        "##################################################################################\n",
        "# TODO: Use 'torchvision.datasets.CIFAR-10' class for loading CIFAR-10 dataset.  #\n",
        "# This dataset has 50000 data for training and 10000 data for test and every     #\n",
        "# data has shape (3*32*32).                                                      #\n",
        "# Also Use 'torchvision.transforms.Compose' for common image transformations     #\n",
        "# such as normalization and use 'torch.utils.data.DataLoader' class that it      #\n",
        "# represents a Python iterable over a dataset and divides data to Batches.       #\n",
        "# Then Split data into 3 part: Train, Validation and Test. Finally,              #\n",
        "# save iterable data in 'trainloader', 'validationloader', 'testloader'.         #\n",
        "##################################################################################\n",
        "\n",
        "\n",
        "##################################################################################\n",
        "#                               End of your code                                 #\n",
        "##################################################################################\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/gdrive/data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12a8927bd3374e3e9ea651ad814e9c9e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /content/gdrive/data/cifar-10-python.tar.gz to /content/gdrive/data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdLQ8BpxEoZ-"
      },
      "source": [
        "#### 3.2. Load Data Test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaOeLN3klZ9F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5d6880d8-e323-45fc-aee8-6cd16cb03024"
      },
      "source": [
        "############################################################\n",
        "# Run the following code an check the size of each batch   #\n",
        "############################################################\n",
        "examples = enumerate(trainloader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "print('The size and type of each batch in ''trainloader'' is:')\n",
        "print(example_data.size())\n",
        "print(type(example_data))\n",
        "examples = enumerate(testloader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "print('\\nThe size and type of each batch in ''testloader'' is:')\n",
        "print(example_data.size())\n",
        "print(type(example_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size and type of each batch in trainloader is:\n",
            "torch.Size([100, 3, 32, 32])\n",
            "<class 'torch.Tensor'>\n",
            "\n",
            "The size and type of each batch in testloader is:\n",
            "torch.Size([100, 3, 32, 32])\n",
            "<class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPMpTd230hLY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "12d66dc2-09df-4fdd-d82c-d3681e4af1ac"
      },
      "source": [
        "#####################################################################\n",
        "# Run the following code and see some of the samples in the dataset #\n",
        "#####################################################################\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images:\n",
        "for i in range(4):\n",
        "    img=torchvision.utils.make_grid(images[i])\n",
        "    print(type(img))\n",
        "    print(img.size())\n",
        "    ###########################################################\n",
        "    #  If you normalize data , here unnormalize them to see   # \n",
        "    #  clear them.                                            #\n",
        "    ###########################################################\n",
        "    m=0.5\n",
        "    s=0.5\n",
        "    img = img *s+m    # unnormalize\n",
        "    ###########################################################\n",
        "    #                   End of your code                      #\n",
        "    ###########################################################\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2, 0)))\n",
        "    plt.title(\"Target Labels: {}\".format(classes[labels[i]]))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([3, 32, 32])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([3, 32, 32])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([3, 32, 32])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([3, 32, 32])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKpijVi0E1Zm"
      },
      "source": [
        "#### 3.3. Network Design:\n",
        "Design the layer of your network and select proper hyperparameter. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8nfR7jqxTBO"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "######################################################################\n",
        "# TODO: Use 'torch.nn' module to design your network for CIFAR-10    #\n",
        "# classification. You have to implement the structure of MLP for it. #\n",
        "# In your design you don't have any limitation and you can use       #\n",
        "# Batch-norm layers, Drop-out layers and etc for generalization      #\n",
        "# improvement (if needed). Use classes and modules from 'torch.nn'.  #\n",
        "# In the following code, the 'MLP' class is your MLP network and     #\n",
        "# this class is inherited from nn.Module, so you can benefit         #\n",
        "# properties of the 'nn.Module'.You may complete '__init__()'        #\n",
        "# constructor by some classes like 'nn.ReLU()' or 'nn.Linear()'      #\n",
        "# to use them in the forward pass of your network.                   #\n",
        "######################################################################\n",
        "  \n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(3*32*32, 1000),\n",
        "            nn.BatchNorm1d(1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1000, 500),\n",
        "            nn.BatchNorm1d(500),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(500, 100),\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(100, 10),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "######################################################################\n",
        "#                          End of your code                          #\n",
        "######################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dGyL8c8E-Rb"
      },
      "source": [
        "#### 3.4. Optimization Algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXuefQ1GB7Ry"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "#############################################################################\n",
        "# TODO: Use a Classification Cross-Entropy loss.Then, use 'torch.optim'     #\n",
        "# module to optimize Cross-Entropy loss. You should select a optimization   #\n",
        "# algorithm and its hyperparameters like learning rate.                     #\n",
        "#############################################################################\n",
        "net = MLP()\n",
        "learning_rate = 1e-3\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "#############################################################################\n",
        "#                             End of your code                              #\n",
        "#############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jL9GEnZFN-z"
      },
      "source": [
        "#### 3.5. Training:\n",
        "You have to tweak `hidden_dim`, `leanirng_rate`, `weight_scale`, `num_epochs` and `reg` and etc to get a validation accuracy above 50%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJdyD46TZY0t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4248bbcb-8216-47b4-cf31-dda58d82ce1b"
      },
      "source": [
        "#######################################################\n",
        "# TODO: Feed the inputs data to the MLP network and   #\n",
        "# optimize Cross-Entropy loss by using target labels. #\n",
        "# Then update weights and biases.                     #\n",
        "#######################################################\n",
        "\n",
        "num_epochs=100\n",
        "num_batchs = len(trainloader)\n",
        "for epoch in range(num_epochs):\n",
        "    total_train=0\n",
        "    correct_train=0\n",
        "    running_loss = 0.0\n",
        "    for batch, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass:\n",
        "        outputs = net.forward(inputs)\n",
        "\n",
        "        # backward pass:\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # optimization:\n",
        "        optimizer.step()\n",
        "        #############################################\n",
        "        #           End of your code                #\n",
        "        #############################################\n",
        "        \n",
        "\n",
        "        # Results: \n",
        "        running_loss += loss.item()\n",
        "\n",
        "        total_train += labels.size(0)\n",
        "        _, predicted_train = torch.max(outputs.data, 1)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "\n",
        "        if batch % (num_batchs/10) == ((num_batchs/10) -1):\n",
        "            print('[Batch %d / %d] loss: %.3f' %\n",
        "                  (batch + 1, num_batchs, running_loss / (num_batchs/10)))\n",
        "            running_loss = 0.0\n",
        "            torch.save(net.state_dict(), './model.pth')\n",
        "            torch.save(optimizer.state_dict(), './optimizer.pth')\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in validationloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    val_acc = correct / total\n",
        "    train_acc = correct_train / total_train\n",
        "    print('(Epoch %d / %d) train acc: %.2f%%; val_acc: %.2f%%' % (\n",
        "          epoch+1, num_epochs, 100*train_acc, 100*val_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Batch 45 / 450] loss: 2.243\n",
            "[Batch 90 / 450] loss: 2.190\n",
            "[Batch 135 / 450] loss: 2.145\n",
            "[Batch 180 / 450] loss: 2.140\n",
            "[Batch 225 / 450] loss: 2.129\n",
            "[Batch 270 / 450] loss: 2.110\n",
            "[Batch 315 / 450] loss: 2.110\n",
            "[Batch 360 / 450] loss: 2.110\n",
            "[Batch 405 / 450] loss: 2.099\n",
            "[Batch 450 / 450] loss: 2.092\n",
            "(Epoch 1 / 100) train acc: 32.78%; val_acc: 38.02%\n",
            "[Batch 45 / 450] loss: 2.079\n",
            "[Batch 90 / 450] loss: 2.083\n",
            "[Batch 135 / 450] loss: 2.070\n",
            "[Batch 180 / 450] loss: 2.077\n",
            "[Batch 225 / 450] loss: 2.070\n",
            "[Batch 270 / 450] loss: 2.069\n",
            "[Batch 315 / 450] loss: 2.073\n",
            "[Batch 360 / 450] loss: 2.059\n",
            "[Batch 405 / 450] loss: 2.062\n",
            "[Batch 450 / 450] loss: 2.058\n",
            "(Epoch 2 / 100) train acc: 39.01%; val_acc: 41.30%\n",
            "[Batch 45 / 450] loss: 2.045\n",
            "[Batch 90 / 450] loss: 2.045\n",
            "[Batch 135 / 450] loss: 2.045\n",
            "[Batch 180 / 450] loss: 2.036\n",
            "[Batch 225 / 450] loss: 2.042\n",
            "[Batch 270 / 450] loss: 2.045\n",
            "[Batch 315 / 450] loss: 2.038\n",
            "[Batch 360 / 450] loss: 2.036\n",
            "[Batch 405 / 450] loss: 2.044\n",
            "[Batch 450 / 450] loss: 2.039\n",
            "(Epoch 3 / 100) train acc: 41.81%; val_acc: 43.10%\n",
            "[Batch 45 / 450] loss: 2.022\n",
            "[Batch 90 / 450] loss: 2.045\n",
            "[Batch 135 / 450] loss: 2.032\n",
            "[Batch 180 / 450] loss: 2.035\n",
            "[Batch 225 / 450] loss: 2.027\n",
            "[Batch 270 / 450] loss: 2.023\n",
            "[Batch 315 / 450] loss: 2.015\n",
            "[Batch 360 / 450] loss: 2.031\n",
            "[Batch 405 / 450] loss: 2.018\n",
            "[Batch 450 / 450] loss: 2.029\n",
            "(Epoch 4 / 100) train acc: 43.14%; val_acc: 42.78%\n",
            "[Batch 45 / 450] loss: 2.007\n",
            "[Batch 90 / 450] loss: 2.018\n",
            "[Batch 135 / 450] loss: 2.022\n",
            "[Batch 180 / 450] loss: 2.013\n",
            "[Batch 225 / 450] loss: 2.025\n",
            "[Batch 270 / 450] loss: 2.012\n",
            "[Batch 315 / 450] loss: 2.010\n",
            "[Batch 360 / 450] loss: 2.024\n",
            "[Batch 405 / 450] loss: 2.016\n",
            "[Batch 450 / 450] loss: 2.014\n",
            "(Epoch 5 / 100) train acc: 44.18%; val_acc: 43.28%\n",
            "[Batch 45 / 450] loss: 2.004\n",
            "[Batch 90 / 450] loss: 2.005\n",
            "[Batch 135 / 450] loss: 2.007\n",
            "[Batch 180 / 450] loss: 2.004\n",
            "[Batch 225 / 450] loss: 2.012\n",
            "[Batch 270 / 450] loss: 2.013\n",
            "[Batch 315 / 450] loss: 2.015\n",
            "[Batch 360 / 450] loss: 1.997\n",
            "[Batch 405 / 450] loss: 1.996\n",
            "[Batch 450 / 450] loss: 2.007\n",
            "(Epoch 6 / 100) train acc: 45.14%; val_acc: 44.30%\n",
            "[Batch 45 / 450] loss: 1.994\n",
            "[Batch 90 / 450] loss: 2.004\n",
            "[Batch 135 / 450] loss: 1.991\n",
            "[Batch 180 / 450] loss: 1.994\n",
            "[Batch 225 / 450] loss: 2.004\n",
            "[Batch 270 / 450] loss: 2.007\n",
            "[Batch 315 / 450] loss: 1.988\n",
            "[Batch 360 / 450] loss: 2.012\n",
            "[Batch 405 / 450] loss: 1.993\n",
            "[Batch 450 / 450] loss: 1.992\n",
            "(Epoch 7 / 100) train acc: 46.11%; val_acc: 44.94%\n",
            "[Batch 45 / 450] loss: 1.990\n",
            "[Batch 90 / 450] loss: 1.984\n",
            "[Batch 135 / 450] loss: 1.989\n",
            "[Batch 180 / 450] loss: 2.001\n",
            "[Batch 225 / 450] loss: 1.996\n",
            "[Batch 270 / 450] loss: 1.995\n",
            "[Batch 315 / 450] loss: 2.010\n",
            "[Batch 360 / 450] loss: 2.000\n",
            "[Batch 405 / 450] loss: 1.987\n",
            "[Batch 450 / 450] loss: 1.980\n",
            "(Epoch 8 / 100) train acc: 46.40%; val_acc: 44.96%\n",
            "[Batch 45 / 450] loss: 1.995\n",
            "[Batch 90 / 450] loss: 1.990\n",
            "[Batch 135 / 450] loss: 1.977\n",
            "[Batch 180 / 450] loss: 1.982\n",
            "[Batch 225 / 450] loss: 1.980\n",
            "[Batch 270 / 450] loss: 1.999\n",
            "[Batch 315 / 450] loss: 1.990\n",
            "[Batch 360 / 450] loss: 1.998\n",
            "[Batch 405 / 450] loss: 1.990\n",
            "[Batch 450 / 450] loss: 1.986\n",
            "(Epoch 9 / 100) train acc: 46.90%; val_acc: 44.78%\n",
            "[Batch 45 / 450] loss: 1.978\n",
            "[Batch 90 / 450] loss: 1.975\n",
            "[Batch 135 / 450] loss: 1.980\n",
            "[Batch 180 / 450] loss: 1.992\n",
            "[Batch 225 / 450] loss: 1.971\n",
            "[Batch 270 / 450] loss: 1.970\n",
            "[Batch 315 / 450] loss: 1.988\n",
            "[Batch 360 / 450] loss: 1.999\n",
            "[Batch 405 / 450] loss: 1.985\n",
            "[Batch 450 / 450] loss: 1.988\n",
            "(Epoch 10 / 100) train acc: 47.52%; val_acc: 46.30%\n",
            "[Batch 45 / 450] loss: 1.979\n",
            "[Batch 90 / 450] loss: 1.971\n",
            "[Batch 135 / 450] loss: 1.973\n",
            "[Batch 180 / 450] loss: 1.975\n",
            "[Batch 225 / 450] loss: 1.985\n",
            "[Batch 270 / 450] loss: 1.980\n",
            "[Batch 315 / 450] loss: 1.979\n",
            "[Batch 360 / 450] loss: 1.988\n",
            "[Batch 405 / 450] loss: 1.978\n",
            "[Batch 450 / 450] loss: 1.966\n",
            "(Epoch 11 / 100) train acc: 48.07%; val_acc: 45.02%\n",
            "[Batch 45 / 450] loss: 1.966\n",
            "[Batch 90 / 450] loss: 1.975\n",
            "[Batch 135 / 450] loss: 1.982\n",
            "[Batch 180 / 450] loss: 1.973\n",
            "[Batch 225 / 450] loss: 1.973\n",
            "[Batch 270 / 450] loss: 1.968\n",
            "[Batch 315 / 450] loss: 1.975\n",
            "[Batch 360 / 450] loss: 1.974\n",
            "[Batch 405 / 450] loss: 1.989\n",
            "[Batch 450 / 450] loss: 1.966\n",
            "(Epoch 12 / 100) train acc: 48.42%; val_acc: 46.08%\n",
            "[Batch 45 / 450] loss: 1.976\n",
            "[Batch 90 / 450] loss: 1.972\n",
            "[Batch 135 / 450] loss: 1.972\n",
            "[Batch 180 / 450] loss: 1.968\n",
            "[Batch 225 / 450] loss: 1.981\n",
            "[Batch 270 / 450] loss: 1.974\n",
            "[Batch 315 / 450] loss: 1.960\n",
            "[Batch 360 / 450] loss: 1.977\n",
            "[Batch 405 / 450] loss: 1.958\n",
            "[Batch 450 / 450] loss: 1.974\n",
            "(Epoch 13 / 100) train acc: 48.68%; val_acc: 46.76%\n",
            "[Batch 45 / 450] loss: 1.961\n",
            "[Batch 90 / 450] loss: 1.970\n",
            "[Batch 135 / 450] loss: 1.963\n",
            "[Batch 180 / 450] loss: 1.963\n",
            "[Batch 225 / 450] loss: 1.955\n",
            "[Batch 270 / 450] loss: 1.959\n",
            "[Batch 315 / 450] loss: 1.973\n",
            "[Batch 360 / 450] loss: 1.961\n",
            "[Batch 405 / 450] loss: 1.973\n",
            "[Batch 450 / 450] loss: 1.966\n",
            "(Epoch 14 / 100) train acc: 49.43%; val_acc: 47.58%\n",
            "[Batch 45 / 450] loss: 1.967\n",
            "[Batch 90 / 450] loss: 1.961\n",
            "[Batch 135 / 450] loss: 1.968\n",
            "[Batch 180 / 450] loss: 1.952\n",
            "[Batch 225 / 450] loss: 1.969\n",
            "[Batch 270 / 450] loss: 1.969\n",
            "[Batch 315 / 450] loss: 1.949\n",
            "[Batch 360 / 450] loss: 1.947\n",
            "[Batch 405 / 450] loss: 1.971\n",
            "[Batch 450 / 450] loss: 1.951\n",
            "(Epoch 15 / 100) train acc: 49.88%; val_acc: 47.00%\n",
            "[Batch 45 / 450] loss: 1.963\n",
            "[Batch 90 / 450] loss: 1.956\n",
            "[Batch 135 / 450] loss: 1.961\n",
            "[Batch 180 / 450] loss: 1.957\n",
            "[Batch 225 / 450] loss: 1.951\n",
            "[Batch 270 / 450] loss: 1.960\n",
            "[Batch 315 / 450] loss: 1.957\n",
            "[Batch 360 / 450] loss: 1.956\n",
            "[Batch 405 / 450] loss: 1.958\n",
            "[Batch 450 / 450] loss: 1.950\n",
            "(Epoch 16 / 100) train acc: 50.04%; val_acc: 47.84%\n",
            "[Batch 45 / 450] loss: 1.942\n",
            "[Batch 90 / 450] loss: 1.949\n",
            "[Batch 135 / 450] loss: 1.948\n",
            "[Batch 180 / 450] loss: 1.958\n",
            "[Batch 225 / 450] loss: 1.965\n",
            "[Batch 270 / 450] loss: 1.954\n",
            "[Batch 315 / 450] loss: 1.951\n",
            "[Batch 360 / 450] loss: 1.955\n",
            "[Batch 405 / 450] loss: 1.956\n",
            "[Batch 450 / 450] loss: 1.940\n",
            "(Epoch 17 / 100) train acc: 50.72%; val_acc: 47.58%\n",
            "[Batch 45 / 450] loss: 1.945\n",
            "[Batch 90 / 450] loss: 1.956\n",
            "[Batch 135 / 450] loss: 1.961\n",
            "[Batch 180 / 450] loss: 1.951\n",
            "[Batch 225 / 450] loss: 1.935\n",
            "[Batch 270 / 450] loss: 1.958\n",
            "[Batch 315 / 450] loss: 1.949\n",
            "[Batch 360 / 450] loss: 1.945\n",
            "[Batch 405 / 450] loss: 1.953\n",
            "[Batch 450 / 450] loss: 1.954\n",
            "(Epoch 18 / 100) train acc: 50.82%; val_acc: 48.16%\n",
            "[Batch 45 / 450] loss: 1.942\n",
            "[Batch 90 / 450] loss: 1.948\n",
            "[Batch 135 / 450] loss: 1.949\n",
            "[Batch 180 / 450] loss: 1.949\n",
            "[Batch 225 / 450] loss: 1.948\n",
            "[Batch 270 / 450] loss: 1.936\n",
            "[Batch 315 / 450] loss: 1.959\n",
            "[Batch 360 / 450] loss: 1.951\n",
            "[Batch 405 / 450] loss: 1.947\n",
            "[Batch 450 / 450] loss: 1.949\n",
            "(Epoch 19 / 100) train acc: 50.93%; val_acc: 47.22%\n",
            "[Batch 45 / 450] loss: 1.943\n",
            "[Batch 90 / 450] loss: 1.947\n",
            "[Batch 135 / 450] loss: 1.936\n",
            "[Batch 180 / 450] loss: 1.941\n",
            "[Batch 225 / 450] loss: 1.944\n",
            "[Batch 270 / 450] loss: 1.941\n",
            "[Batch 315 / 450] loss: 1.952\n",
            "[Batch 360 / 450] loss: 1.944\n",
            "[Batch 405 / 450] loss: 1.948\n",
            "[Batch 450 / 450] loss: 1.954\n",
            "(Epoch 20 / 100) train acc: 51.44%; val_acc: 47.88%\n",
            "[Batch 45 / 450] loss: 1.933\n",
            "[Batch 90 / 450] loss: 1.934\n",
            "[Batch 135 / 450] loss: 1.936\n",
            "[Batch 180 / 450] loss: 1.943\n",
            "[Batch 225 / 450] loss: 1.935\n",
            "[Batch 270 / 450] loss: 1.948\n",
            "[Batch 315 / 450] loss: 1.946\n",
            "[Batch 360 / 450] loss: 1.936\n",
            "[Batch 405 / 450] loss: 1.947\n",
            "[Batch 450 / 450] loss: 1.936\n",
            "(Epoch 21 / 100) train acc: 51.94%; val_acc: 48.16%\n",
            "[Batch 45 / 450] loss: 1.937\n",
            "[Batch 90 / 450] loss: 1.935\n",
            "[Batch 135 / 450] loss: 1.941\n",
            "[Batch 180 / 450] loss: 1.948\n",
            "[Batch 225 / 450] loss: 1.944\n",
            "[Batch 270 / 450] loss: 1.943\n",
            "[Batch 315 / 450] loss: 1.935\n",
            "[Batch 360 / 450] loss: 1.921\n",
            "[Batch 405 / 450] loss: 1.949\n",
            "[Batch 450 / 450] loss: 1.942\n",
            "(Epoch 22 / 100) train acc: 51.81%; val_acc: 48.44%\n",
            "[Batch 45 / 450] loss: 1.924\n",
            "[Batch 90 / 450] loss: 1.930\n",
            "[Batch 135 / 450] loss: 1.941\n",
            "[Batch 180 / 450] loss: 1.943\n",
            "[Batch 225 / 450] loss: 1.930\n",
            "[Batch 270 / 450] loss: 1.947\n",
            "[Batch 315 / 450] loss: 1.933\n",
            "[Batch 360 / 450] loss: 1.932\n",
            "[Batch 405 / 450] loss: 1.943\n",
            "[Batch 450 / 450] loss: 1.932\n",
            "(Epoch 23 / 100) train acc: 52.34%; val_acc: 48.10%\n",
            "[Batch 45 / 450] loss: 1.915\n",
            "[Batch 90 / 450] loss: 1.939\n",
            "[Batch 135 / 450] loss: 1.938\n",
            "[Batch 180 / 450] loss: 1.947\n",
            "[Batch 225 / 450] loss: 1.932\n",
            "[Batch 270 / 450] loss: 1.940\n",
            "[Batch 315 / 450] loss: 1.929\n",
            "[Batch 360 / 450] loss: 1.935\n",
            "[Batch 405 / 450] loss: 1.931\n",
            "[Batch 450 / 450] loss: 1.936\n",
            "(Epoch 24 / 100) train acc: 52.47%; val_acc: 48.78%\n",
            "[Batch 45 / 450] loss: 1.931\n",
            "[Batch 90 / 450] loss: 1.939\n",
            "[Batch 135 / 450] loss: 1.923\n",
            "[Batch 180 / 450] loss: 1.932\n",
            "[Batch 225 / 450] loss: 1.922\n",
            "[Batch 270 / 450] loss: 1.925\n",
            "[Batch 315 / 450] loss: 1.941\n",
            "[Batch 360 / 450] loss: 1.918\n",
            "[Batch 405 / 450] loss: 1.930\n",
            "[Batch 450 / 450] loss: 1.927\n",
            "(Epoch 25 / 100) train acc: 52.98%; val_acc: 48.32%\n",
            "[Batch 45 / 450] loss: 1.912\n",
            "[Batch 90 / 450] loss: 1.920\n",
            "[Batch 135 / 450] loss: 1.936\n",
            "[Batch 180 / 450] loss: 1.920\n",
            "[Batch 225 / 450] loss: 1.921\n",
            "[Batch 270 / 450] loss: 1.922\n",
            "[Batch 315 / 450] loss: 1.929\n",
            "[Batch 360 / 450] loss: 1.926\n",
            "[Batch 405 / 450] loss: 1.931\n",
            "[Batch 450 / 450] loss: 1.938\n",
            "(Epoch 26 / 100) train acc: 53.36%; val_acc: 48.12%\n",
            "[Batch 45 / 450] loss: 1.927\n",
            "[Batch 90 / 450] loss: 1.918\n",
            "[Batch 135 / 450] loss: 1.932\n",
            "[Batch 180 / 450] loss: 1.912\n",
            "[Batch 225 / 450] loss: 1.917\n",
            "[Batch 270 / 450] loss: 1.912\n",
            "[Batch 315 / 450] loss: 1.934\n",
            "[Batch 360 / 450] loss: 1.926\n",
            "[Batch 405 / 450] loss: 1.926\n",
            "[Batch 450 / 450] loss: 1.932\n",
            "(Epoch 27 / 100) train acc: 53.49%; val_acc: 48.80%\n",
            "[Batch 45 / 450] loss: 1.926\n",
            "[Batch 90 / 450] loss: 1.915\n",
            "[Batch 135 / 450] loss: 1.922\n",
            "[Batch 180 / 450] loss: 1.913\n",
            "[Batch 225 / 450] loss: 1.923\n",
            "[Batch 270 / 450] loss: 1.916\n",
            "[Batch 315 / 450] loss: 1.923\n",
            "[Batch 360 / 450] loss: 1.923\n",
            "[Batch 405 / 450] loss: 1.929\n",
            "[Batch 450 / 450] loss: 1.935\n",
            "(Epoch 28 / 100) train acc: 53.59%; val_acc: 48.98%\n",
            "[Batch 45 / 450] loss: 1.921\n",
            "[Batch 90 / 450] loss: 1.921\n",
            "[Batch 135 / 450] loss: 1.920\n",
            "[Batch 180 / 450] loss: 1.930\n",
            "[Batch 225 / 450] loss: 1.913\n",
            "[Batch 270 / 450] loss: 1.918\n",
            "[Batch 315 / 450] loss: 1.928\n",
            "[Batch 360 / 450] loss: 1.916\n",
            "[Batch 405 / 450] loss: 1.918\n",
            "[Batch 450 / 450] loss: 1.929\n",
            "(Epoch 29 / 100) train acc: 53.78%; val_acc: 48.64%\n",
            "[Batch 45 / 450] loss: 1.921\n",
            "[Batch 90 / 450] loss: 1.926\n",
            "[Batch 135 / 450] loss: 1.931\n",
            "[Batch 180 / 450] loss: 1.921\n",
            "[Batch 225 / 450] loss: 1.913\n",
            "[Batch 270 / 450] loss: 1.927\n",
            "[Batch 315 / 450] loss: 1.914\n",
            "[Batch 360 / 450] loss: 1.923\n",
            "[Batch 405 / 450] loss: 1.920\n",
            "[Batch 450 / 450] loss: 1.915\n",
            "(Epoch 30 / 100) train acc: 53.72%; val_acc: 49.34%\n",
            "[Batch 45 / 450] loss: 1.909\n",
            "[Batch 90 / 450] loss: 1.915\n",
            "[Batch 135 / 450] loss: 1.915\n",
            "[Batch 180 / 450] loss: 1.908\n",
            "[Batch 225 / 450] loss: 1.921\n",
            "[Batch 270 / 450] loss: 1.910\n",
            "[Batch 315 / 450] loss: 1.924\n",
            "[Batch 360 / 450] loss: 1.921\n",
            "[Batch 405 / 450] loss: 1.918\n",
            "[Batch 450 / 450] loss: 1.929\n",
            "(Epoch 31 / 100) train acc: 54.09%; val_acc: 48.94%\n",
            "[Batch 45 / 450] loss: 1.898\n",
            "[Batch 90 / 450] loss: 1.908\n",
            "[Batch 135 / 450] loss: 1.906\n",
            "[Batch 180 / 450] loss: 1.913\n",
            "[Batch 225 / 450] loss: 1.911\n",
            "[Batch 270 / 450] loss: 1.920\n",
            "[Batch 315 / 450] loss: 1.914\n",
            "[Batch 360 / 450] loss: 1.929\n",
            "[Batch 405 / 450] loss: 1.910\n",
            "[Batch 450 / 450] loss: 1.915\n",
            "(Epoch 32 / 100) train acc: 54.68%; val_acc: 49.56%\n",
            "[Batch 45 / 450] loss: 1.922\n",
            "[Batch 90 / 450] loss: 1.907\n",
            "[Batch 135 / 450] loss: 1.903\n",
            "[Batch 180 / 450] loss: 1.909\n",
            "[Batch 225 / 450] loss: 1.917\n",
            "[Batch 270 / 450] loss: 1.903\n",
            "[Batch 315 / 450] loss: 1.902\n",
            "[Batch 360 / 450] loss: 1.914\n",
            "[Batch 405 / 450] loss: 1.916\n",
            "[Batch 450 / 450] loss: 1.924\n",
            "(Epoch 33 / 100) train acc: 54.72%; val_acc: 49.16%\n",
            "[Batch 45 / 450] loss: 1.911\n",
            "[Batch 90 / 450] loss: 1.901\n",
            "[Batch 135 / 450] loss: 1.906\n",
            "[Batch 180 / 450] loss: 1.930\n",
            "[Batch 225 / 450] loss: 1.910\n",
            "[Batch 270 / 450] loss: 1.914\n",
            "[Batch 315 / 450] loss: 1.902\n",
            "[Batch 360 / 450] loss: 1.908\n",
            "[Batch 405 / 450] loss: 1.903\n",
            "[Batch 450 / 450] loss: 1.913\n",
            "(Epoch 34 / 100) train acc: 54.88%; val_acc: 48.84%\n",
            "[Batch 45 / 450] loss: 1.915\n",
            "[Batch 90 / 450] loss: 1.913\n",
            "[Batch 135 / 450] loss: 1.904\n",
            "[Batch 180 / 450] loss: 1.908\n",
            "[Batch 225 / 450] loss: 1.925\n",
            "[Batch 270 / 450] loss: 1.912\n",
            "[Batch 315 / 450] loss: 1.910\n",
            "[Batch 360 / 450] loss: 1.905\n",
            "[Batch 405 / 450] loss: 1.900\n",
            "[Batch 450 / 450] loss: 1.917\n",
            "(Epoch 35 / 100) train acc: 54.75%; val_acc: 49.68%\n",
            "[Batch 45 / 450] loss: 1.889\n",
            "[Batch 90 / 450] loss: 1.908\n",
            "[Batch 135 / 450] loss: 1.912\n",
            "[Batch 180 / 450] loss: 1.896\n",
            "[Batch 225 / 450] loss: 1.909\n",
            "[Batch 270 / 450] loss: 1.911\n",
            "[Batch 315 / 450] loss: 1.909\n",
            "[Batch 360 / 450] loss: 1.911\n",
            "[Batch 405 / 450] loss: 1.900\n",
            "[Batch 450 / 450] loss: 1.909\n",
            "(Epoch 36 / 100) train acc: 55.30%; val_acc: 49.66%\n",
            "[Batch 45 / 450] loss: 1.916\n",
            "[Batch 90 / 450] loss: 1.908\n",
            "[Batch 135 / 450] loss: 1.901\n",
            "[Batch 180 / 450] loss: 1.904\n",
            "[Batch 225 / 450] loss: 1.906\n",
            "[Batch 270 / 450] loss: 1.904\n",
            "[Batch 315 / 450] loss: 1.896\n",
            "[Batch 360 / 450] loss: 1.911\n",
            "[Batch 405 / 450] loss: 1.892\n",
            "[Batch 450 / 450] loss: 1.905\n",
            "(Epoch 37 / 100) train acc: 55.36%; val_acc: 49.36%\n",
            "[Batch 45 / 450] loss: 1.895\n",
            "[Batch 90 / 450] loss: 1.907\n",
            "[Batch 135 / 450] loss: 1.905\n",
            "[Batch 180 / 450] loss: 1.889\n",
            "[Batch 225 / 450] loss: 1.911\n",
            "[Batch 270 / 450] loss: 1.895\n",
            "[Batch 315 / 450] loss: 1.904\n",
            "[Batch 360 / 450] loss: 1.910\n",
            "[Batch 405 / 450] loss: 1.908\n",
            "[Batch 450 / 450] loss: 1.907\n",
            "(Epoch 38 / 100) train acc: 55.55%; val_acc: 48.90%\n",
            "[Batch 45 / 450] loss: 1.894\n",
            "[Batch 90 / 450] loss: 1.892\n",
            "[Batch 135 / 450] loss: 1.905\n",
            "[Batch 180 / 450] loss: 1.907\n",
            "[Batch 225 / 450] loss: 1.898\n",
            "[Batch 270 / 450] loss: 1.893\n",
            "[Batch 315 / 450] loss: 1.905\n",
            "[Batch 360 / 450] loss: 1.911\n",
            "[Batch 405 / 450] loss: 1.902\n",
            "[Batch 450 / 450] loss: 1.903\n",
            "(Epoch 39 / 100) train acc: 55.87%; val_acc: 49.16%\n",
            "[Batch 45 / 450] loss: 1.904\n",
            "[Batch 90 / 450] loss: 1.893\n",
            "[Batch 135 / 450] loss: 1.888\n",
            "[Batch 180 / 450] loss: 1.896\n",
            "[Batch 225 / 450] loss: 1.895\n",
            "[Batch 270 / 450] loss: 1.907\n",
            "[Batch 315 / 450] loss: 1.901\n",
            "[Batch 360 / 450] loss: 1.898\n",
            "[Batch 405 / 450] loss: 1.897\n",
            "[Batch 450 / 450] loss: 1.887\n",
            "(Epoch 40 / 100) train acc: 56.28%; val_acc: 49.26%\n",
            "[Batch 45 / 450] loss: 1.889\n",
            "[Batch 90 / 450] loss: 1.884\n",
            "[Batch 135 / 450] loss: 1.914\n",
            "[Batch 180 / 450] loss: 1.895\n",
            "[Batch 225 / 450] loss: 1.912\n",
            "[Batch 270 / 450] loss: 1.890\n",
            "[Batch 315 / 450] loss: 1.903\n",
            "[Batch 360 / 450] loss: 1.900\n",
            "[Batch 405 / 450] loss: 1.903\n",
            "[Batch 450 / 450] loss: 1.892\n",
            "(Epoch 41 / 100) train acc: 56.02%; val_acc: 49.76%\n",
            "[Batch 45 / 450] loss: 1.888\n",
            "[Batch 90 / 450] loss: 1.892\n",
            "[Batch 135 / 450] loss: 1.897\n",
            "[Batch 180 / 450] loss: 1.892\n",
            "[Batch 225 / 450] loss: 1.888\n",
            "[Batch 270 / 450] loss: 1.903\n",
            "[Batch 315 / 450] loss: 1.896\n",
            "[Batch 360 / 450] loss: 1.902\n",
            "[Batch 405 / 450] loss: 1.904\n",
            "[Batch 450 / 450] loss: 1.896\n",
            "(Epoch 42 / 100) train acc: 56.34%; val_acc: 50.92%\n",
            "[Batch 45 / 450] loss: 1.885\n",
            "[Batch 90 / 450] loss: 1.905\n",
            "[Batch 135 / 450] loss: 1.883\n",
            "[Batch 180 / 450] loss: 1.890\n",
            "[Batch 225 / 450] loss: 1.893\n",
            "[Batch 270 / 450] loss: 1.891\n",
            "[Batch 315 / 450] loss: 1.886\n",
            "[Batch 360 / 450] loss: 1.902\n",
            "[Batch 405 / 450] loss: 1.899\n",
            "[Batch 450 / 450] loss: 1.883\n",
            "(Epoch 43 / 100) train acc: 56.73%; val_acc: 49.84%\n",
            "[Batch 45 / 450] loss: 1.887\n",
            "[Batch 90 / 450] loss: 1.892\n",
            "[Batch 135 / 450] loss: 1.897\n",
            "[Batch 180 / 450] loss: 1.891\n",
            "[Batch 225 / 450] loss: 1.894\n",
            "[Batch 270 / 450] loss: 1.891\n",
            "[Batch 315 / 450] loss: 1.887\n",
            "[Batch 360 / 450] loss: 1.888\n",
            "[Batch 405 / 450] loss: 1.893\n",
            "[Batch 450 / 450] loss: 1.902\n",
            "(Epoch 44 / 100) train acc: 56.64%; val_acc: 50.48%\n",
            "[Batch 45 / 450] loss: 1.877\n",
            "[Batch 90 / 450] loss: 1.887\n",
            "[Batch 135 / 450] loss: 1.900\n",
            "[Batch 180 / 450] loss: 1.898\n",
            "[Batch 225 / 450] loss: 1.895\n",
            "[Batch 270 / 450] loss: 1.887\n",
            "[Batch 315 / 450] loss: 1.890\n",
            "[Batch 360 / 450] loss: 1.903\n",
            "[Batch 405 / 450] loss: 1.892\n",
            "[Batch 450 / 450] loss: 1.884\n",
            "(Epoch 45 / 100) train acc: 56.76%; val_acc: 50.24%\n",
            "[Batch 45 / 450] loss: 1.892\n",
            "[Batch 90 / 450] loss: 1.880\n",
            "[Batch 135 / 450] loss: 1.872\n",
            "[Batch 180 / 450] loss: 1.889\n",
            "[Batch 225 / 450] loss: 1.901\n",
            "[Batch 270 / 450] loss: 1.895\n",
            "[Batch 315 / 450] loss: 1.883\n",
            "[Batch 360 / 450] loss: 1.893\n",
            "[Batch 405 / 450] loss: 1.892\n",
            "[Batch 450 / 450] loss: 1.881\n",
            "(Epoch 46 / 100) train acc: 57.14%; val_acc: 50.00%\n",
            "[Batch 45 / 450] loss: 1.882\n",
            "[Batch 90 / 450] loss: 1.890\n",
            "[Batch 135 / 450] loss: 1.895\n",
            "[Batch 180 / 450] loss: 1.887\n",
            "[Batch 225 / 450] loss: 1.882\n",
            "[Batch 270 / 450] loss: 1.899\n",
            "[Batch 315 / 450] loss: 1.881\n",
            "[Batch 360 / 450] loss: 1.895\n",
            "[Batch 405 / 450] loss: 1.880\n",
            "[Batch 450 / 450] loss: 1.885\n",
            "(Epoch 47 / 100) train acc: 57.18%; val_acc: 50.24%\n",
            "[Batch 45 / 450] loss: 1.884\n",
            "[Batch 90 / 450] loss: 1.882\n",
            "[Batch 135 / 450] loss: 1.875\n",
            "[Batch 180 / 450] loss: 1.890\n",
            "[Batch 225 / 450] loss: 1.886\n",
            "[Batch 270 / 450] loss: 1.888\n",
            "[Batch 315 / 450] loss: 1.902\n",
            "[Batch 360 / 450] loss: 1.887\n",
            "[Batch 405 / 450] loss: 1.887\n",
            "[Batch 450 / 450] loss: 1.882\n",
            "(Epoch 48 / 100) train acc: 57.28%; val_acc: 50.18%\n",
            "[Batch 45 / 450] loss: 1.874\n",
            "[Batch 90 / 450] loss: 1.887\n",
            "[Batch 135 / 450] loss: 1.881\n",
            "[Batch 180 / 450] loss: 1.879\n",
            "[Batch 225 / 450] loss: 1.878\n",
            "[Batch 270 / 450] loss: 1.895\n",
            "[Batch 315 / 450] loss: 1.900\n",
            "[Batch 360 / 450] loss: 1.885\n",
            "[Batch 405 / 450] loss: 1.888\n",
            "[Batch 450 / 450] loss: 1.886\n",
            "(Epoch 49 / 100) train acc: 57.36%; val_acc: 50.14%\n",
            "[Batch 45 / 450] loss: 1.878\n",
            "[Batch 90 / 450] loss: 1.882\n",
            "[Batch 135 / 450] loss: 1.886\n",
            "[Batch 180 / 450] loss: 1.880\n",
            "[Batch 225 / 450] loss: 1.867\n",
            "[Batch 270 / 450] loss: 1.896\n",
            "[Batch 315 / 450] loss: 1.887\n",
            "[Batch 360 / 450] loss: 1.873\n",
            "[Batch 405 / 450] loss: 1.889\n",
            "[Batch 450 / 450] loss: 1.878\n",
            "(Epoch 50 / 100) train acc: 57.70%; val_acc: 51.08%\n",
            "[Batch 45 / 450] loss: 1.875\n",
            "[Batch 90 / 450] loss: 1.886\n",
            "[Batch 135 / 450] loss: 1.877\n",
            "[Batch 180 / 450] loss: 1.881\n",
            "[Batch 225 / 450] loss: 1.886\n",
            "[Batch 270 / 450] loss: 1.876\n",
            "[Batch 315 / 450] loss: 1.885\n",
            "[Batch 360 / 450] loss: 1.887\n",
            "[Batch 405 / 450] loss: 1.881\n",
            "[Batch 450 / 450] loss: 1.884\n",
            "(Epoch 51 / 100) train acc: 57.68%; val_acc: 50.60%\n",
            "[Batch 45 / 450] loss: 1.870\n",
            "[Batch 90 / 450] loss: 1.882\n",
            "[Batch 135 / 450] loss: 1.870\n",
            "[Batch 180 / 450] loss: 1.886\n",
            "[Batch 225 / 450] loss: 1.889\n",
            "[Batch 270 / 450] loss: 1.881\n",
            "[Batch 315 / 450] loss: 1.884\n",
            "[Batch 360 / 450] loss: 1.891\n",
            "[Batch 405 / 450] loss: 1.888\n",
            "[Batch 450 / 450] loss: 1.871\n",
            "(Epoch 52 / 100) train acc: 57.74%; val_acc: 50.62%\n",
            "[Batch 45 / 450] loss: 1.872\n",
            "[Batch 90 / 450] loss: 1.873\n",
            "[Batch 135 / 450] loss: 1.878\n",
            "[Batch 180 / 450] loss: 1.867\n",
            "[Batch 225 / 450] loss: 1.870\n",
            "[Batch 270 / 450] loss: 1.871\n",
            "[Batch 315 / 450] loss: 1.875\n",
            "[Batch 360 / 450] loss: 1.879\n",
            "[Batch 405 / 450] loss: 1.887\n",
            "[Batch 450 / 450] loss: 1.879\n",
            "(Epoch 53 / 100) train acc: 58.48%; val_acc: 49.50%\n",
            "[Batch 45 / 450] loss: 1.868\n",
            "[Batch 90 / 450] loss: 1.889\n",
            "[Batch 135 / 450] loss: 1.870\n",
            "[Batch 180 / 450] loss: 1.868\n",
            "[Batch 225 / 450] loss: 1.874\n",
            "[Batch 270 / 450] loss: 1.873\n",
            "[Batch 315 / 450] loss: 1.870\n",
            "[Batch 360 / 450] loss: 1.885\n",
            "[Batch 405 / 450] loss: 1.890\n",
            "[Batch 450 / 450] loss: 1.883\n",
            "(Epoch 54 / 100) train acc: 58.24%; val_acc: 50.54%\n",
            "[Batch 45 / 450] loss: 1.864\n",
            "[Batch 90 / 450] loss: 1.874\n",
            "[Batch 135 / 450] loss: 1.879\n",
            "[Batch 180 / 450] loss: 1.869\n",
            "[Batch 225 / 450] loss: 1.870\n",
            "[Batch 270 / 450] loss: 1.872\n",
            "[Batch 315 / 450] loss: 1.889\n",
            "[Batch 360 / 450] loss: 1.869\n",
            "[Batch 405 / 450] loss: 1.884\n",
            "[Batch 450 / 450] loss: 1.879\n",
            "(Epoch 55 / 100) train acc: 58.43%; val_acc: 50.70%\n",
            "[Batch 45 / 450] loss: 1.867\n",
            "[Batch 90 / 450] loss: 1.870\n",
            "[Batch 135 / 450] loss: 1.871\n",
            "[Batch 180 / 450] loss: 1.880\n",
            "[Batch 225 / 450] loss: 1.873\n",
            "[Batch 270 / 450] loss: 1.879\n",
            "[Batch 315 / 450] loss: 1.870\n",
            "[Batch 360 / 450] loss: 1.860\n",
            "[Batch 405 / 450] loss: 1.865\n",
            "[Batch 450 / 450] loss: 1.885\n",
            "(Epoch 56 / 100) train acc: 58.79%; val_acc: 50.70%\n",
            "[Batch 45 / 450] loss: 1.858\n",
            "[Batch 90 / 450] loss: 1.860\n",
            "[Batch 135 / 450] loss: 1.870\n",
            "[Batch 180 / 450] loss: 1.885\n",
            "[Batch 225 / 450] loss: 1.879\n",
            "[Batch 270 / 450] loss: 1.885\n",
            "[Batch 315 / 450] loss: 1.873\n",
            "[Batch 360 / 450] loss: 1.866\n",
            "[Batch 405 / 450] loss: 1.865\n",
            "[Batch 450 / 450] loss: 1.870\n",
            "(Epoch 57 / 100) train acc: 58.79%; val_acc: 50.86%\n",
            "[Batch 45 / 450] loss: 1.863\n",
            "[Batch 90 / 450] loss: 1.864\n",
            "[Batch 135 / 450] loss: 1.863\n",
            "[Batch 180 / 450] loss: 1.875\n",
            "[Batch 225 / 450] loss: 1.872\n",
            "[Batch 270 / 450] loss: 1.866\n",
            "[Batch 315 / 450] loss: 1.867\n",
            "[Batch 360 / 450] loss: 1.876\n",
            "[Batch 405 / 450] loss: 1.874\n",
            "[Batch 450 / 450] loss: 1.869\n",
            "(Epoch 58 / 100) train acc: 59.08%; val_acc: 50.34%\n",
            "[Batch 45 / 450] loss: 1.856\n",
            "[Batch 90 / 450] loss: 1.866\n",
            "[Batch 135 / 450] loss: 1.867\n",
            "[Batch 180 / 450] loss: 1.872\n",
            "[Batch 225 / 450] loss: 1.866\n",
            "[Batch 270 / 450] loss: 1.861\n",
            "[Batch 315 / 450] loss: 1.877\n",
            "[Batch 360 / 450] loss: 1.874\n",
            "[Batch 405 / 450] loss: 1.879\n",
            "[Batch 450 / 450] loss: 1.870\n",
            "(Epoch 59 / 100) train acc: 59.00%; val_acc: 51.04%\n",
            "[Batch 45 / 450] loss: 1.859\n",
            "[Batch 90 / 450] loss: 1.873\n",
            "[Batch 135 / 450] loss: 1.863\n",
            "[Batch 180 / 450] loss: 1.868\n",
            "[Batch 225 / 450] loss: 1.869\n",
            "[Batch 270 / 450] loss: 1.868\n",
            "[Batch 315 / 450] loss: 1.865\n",
            "[Batch 360 / 450] loss: 1.865\n",
            "[Batch 405 / 450] loss: 1.885\n",
            "[Batch 450 / 450] loss: 1.873\n",
            "(Epoch 60 / 100) train acc: 59.03%; val_acc: 50.76%\n",
            "[Batch 45 / 450] loss: 1.861\n",
            "[Batch 90 / 450] loss: 1.858\n",
            "[Batch 135 / 450] loss: 1.870\n",
            "[Batch 180 / 450] loss: 1.875\n",
            "[Batch 225 / 450] loss: 1.866\n",
            "[Batch 270 / 450] loss: 1.874\n",
            "[Batch 315 / 450] loss: 1.865\n",
            "[Batch 360 / 450] loss: 1.870\n",
            "[Batch 405 / 450] loss: 1.869\n",
            "[Batch 450 / 450] loss: 1.865\n",
            "(Epoch 61 / 100) train acc: 59.28%; val_acc: 50.74%\n",
            "[Batch 45 / 450] loss: 1.862\n",
            "[Batch 90 / 450] loss: 1.861\n",
            "[Batch 135 / 450] loss: 1.879\n",
            "[Batch 180 / 450] loss: 1.878\n",
            "[Batch 225 / 450] loss: 1.856\n",
            "[Batch 270 / 450] loss: 1.873\n",
            "[Batch 315 / 450] loss: 1.866\n",
            "[Batch 360 / 450] loss: 1.859\n",
            "[Batch 405 / 450] loss: 1.858\n",
            "[Batch 450 / 450] loss: 1.885\n",
            "(Epoch 62 / 100) train acc: 59.14%; val_acc: 50.84%\n",
            "[Batch 45 / 450] loss: 1.864\n",
            "[Batch 90 / 450] loss: 1.855\n",
            "[Batch 135 / 450] loss: 1.863\n",
            "[Batch 180 / 450] loss: 1.859\n",
            "[Batch 225 / 450] loss: 1.868\n",
            "[Batch 270 / 450] loss: 1.852\n",
            "[Batch 315 / 450] loss: 1.867\n",
            "[Batch 360 / 450] loss: 1.857\n",
            "[Batch 405 / 450] loss: 1.866\n",
            "[Batch 450 / 450] loss: 1.871\n",
            "(Epoch 63 / 100) train acc: 59.67%; val_acc: 50.66%\n",
            "[Batch 45 / 450] loss: 1.854\n",
            "[Batch 90 / 450] loss: 1.867\n",
            "[Batch 135 / 450] loss: 1.858\n",
            "[Batch 180 / 450] loss: 1.859\n",
            "[Batch 225 / 450] loss: 1.867\n",
            "[Batch 270 / 450] loss: 1.859\n",
            "[Batch 315 / 450] loss: 1.865\n",
            "[Batch 360 / 450] loss: 1.864\n",
            "[Batch 405 / 450] loss: 1.868\n",
            "[Batch 450 / 450] loss: 1.866\n",
            "(Epoch 64 / 100) train acc: 59.68%; val_acc: 50.78%\n",
            "[Batch 45 / 450] loss: 1.853\n",
            "[Batch 90 / 450] loss: 1.868\n",
            "[Batch 135 / 450] loss: 1.865\n",
            "[Batch 180 / 450] loss: 1.862\n",
            "[Batch 225 / 450] loss: 1.867\n",
            "[Batch 270 / 450] loss: 1.856\n",
            "[Batch 315 / 450] loss: 1.867\n",
            "[Batch 360 / 450] loss: 1.869\n",
            "[Batch 405 / 450] loss: 1.868\n",
            "[Batch 450 / 450] loss: 1.860\n",
            "(Epoch 65 / 100) train acc: 59.53%; val_acc: 50.12%\n",
            "[Batch 45 / 450] loss: 1.863\n",
            "[Batch 90 / 450] loss: 1.861\n",
            "[Batch 135 / 450] loss: 1.872\n",
            "[Batch 180 / 450] loss: 1.870\n",
            "[Batch 225 / 450] loss: 1.848\n",
            "[Batch 270 / 450] loss: 1.868\n",
            "[Batch 315 / 450] loss: 1.855\n",
            "[Batch 360 / 450] loss: 1.861\n",
            "[Batch 405 / 450] loss: 1.858\n",
            "[Batch 450 / 450] loss: 1.858\n",
            "(Epoch 66 / 100) train acc: 59.80%; val_acc: 51.26%\n",
            "[Batch 45 / 450] loss: 1.859\n",
            "[Batch 90 / 450] loss: 1.865\n",
            "[Batch 135 / 450] loss: 1.856\n",
            "[Batch 180 / 450] loss: 1.865\n",
            "[Batch 225 / 450] loss: 1.848\n",
            "[Batch 270 / 450] loss: 1.850\n",
            "[Batch 315 / 450] loss: 1.863\n",
            "[Batch 360 / 450] loss: 1.872\n",
            "[Batch 405 / 450] loss: 1.867\n",
            "[Batch 450 / 450] loss: 1.856\n",
            "(Epoch 67 / 100) train acc: 59.94%; val_acc: 52.16%\n",
            "[Batch 45 / 450] loss: 1.849\n",
            "[Batch 90 / 450] loss: 1.850\n",
            "[Batch 135 / 450] loss: 1.856\n",
            "[Batch 180 / 450] loss: 1.865\n",
            "[Batch 225 / 450] loss: 1.852\n",
            "[Batch 270 / 450] loss: 1.865\n",
            "[Batch 315 / 450] loss: 1.864\n",
            "[Batch 360 / 450] loss: 1.860\n",
            "[Batch 405 / 450] loss: 1.862\n",
            "[Batch 450 / 450] loss: 1.867\n",
            "(Epoch 68 / 100) train acc: 59.99%; val_acc: 50.22%\n",
            "[Batch 45 / 450] loss: 1.843\n",
            "[Batch 90 / 450] loss: 1.844\n",
            "[Batch 135 / 450] loss: 1.859\n",
            "[Batch 180 / 450] loss: 1.860\n",
            "[Batch 225 / 450] loss: 1.853\n",
            "[Batch 270 / 450] loss: 1.870\n",
            "[Batch 315 / 450] loss: 1.867\n",
            "[Batch 360 / 450] loss: 1.863\n",
            "[Batch 405 / 450] loss: 1.859\n",
            "[Batch 450 / 450] loss: 1.868\n",
            "(Epoch 69 / 100) train acc: 60.05%; val_acc: 50.12%\n",
            "[Batch 45 / 450] loss: 1.847\n",
            "[Batch 90 / 450] loss: 1.861\n",
            "[Batch 135 / 450] loss: 1.871\n",
            "[Batch 180 / 450] loss: 1.847\n",
            "[Batch 225 / 450] loss: 1.855\n",
            "[Batch 270 / 450] loss: 1.863\n",
            "[Batch 315 / 450] loss: 1.856\n",
            "[Batch 360 / 450] loss: 1.868\n",
            "[Batch 405 / 450] loss: 1.849\n",
            "[Batch 450 / 450] loss: 1.860\n",
            "(Epoch 70 / 100) train acc: 60.18%; val_acc: 51.32%\n",
            "[Batch 45 / 450] loss: 1.852\n",
            "[Batch 90 / 450] loss: 1.864\n",
            "[Batch 135 / 450] loss: 1.849\n",
            "[Batch 180 / 450] loss: 1.861\n",
            "[Batch 225 / 450] loss: 1.852\n",
            "[Batch 270 / 450] loss: 1.864\n",
            "[Batch 315 / 450] loss: 1.862\n",
            "[Batch 360 / 450] loss: 1.865\n",
            "[Batch 405 / 450] loss: 1.846\n",
            "[Batch 450 / 450] loss: 1.846\n",
            "(Epoch 71 / 100) train acc: 60.38%; val_acc: 51.26%\n",
            "[Batch 45 / 450] loss: 1.860\n",
            "[Batch 90 / 450] loss: 1.846\n",
            "[Batch 135 / 450] loss: 1.850\n",
            "[Batch 180 / 450] loss: 1.857\n",
            "[Batch 225 / 450] loss: 1.848\n",
            "[Batch 270 / 450] loss: 1.864\n",
            "[Batch 315 / 450] loss: 1.862\n",
            "[Batch 360 / 450] loss: 1.852\n",
            "[Batch 405 / 450] loss: 1.850\n",
            "[Batch 450 / 450] loss: 1.864\n",
            "(Epoch 72 / 100) train acc: 60.37%; val_acc: 50.64%\n",
            "[Batch 45 / 450] loss: 1.839\n",
            "[Batch 90 / 450] loss: 1.867\n",
            "[Batch 135 / 450] loss: 1.858\n",
            "[Batch 180 / 450] loss: 1.852\n",
            "[Batch 225 / 450] loss: 1.846\n",
            "[Batch 270 / 450] loss: 1.859\n",
            "[Batch 315 / 450] loss: 1.846\n",
            "[Batch 360 / 450] loss: 1.860\n",
            "[Batch 405 / 450] loss: 1.849\n",
            "[Batch 450 / 450] loss: 1.857\n",
            "(Epoch 73 / 100) train acc: 60.67%; val_acc: 51.20%\n",
            "[Batch 45 / 450] loss: 1.861\n",
            "[Batch 90 / 450] loss: 1.852\n",
            "[Batch 135 / 450] loss: 1.855\n",
            "[Batch 180 / 450] loss: 1.849\n",
            "[Batch 225 / 450] loss: 1.859\n",
            "[Batch 270 / 450] loss: 1.852\n",
            "[Batch 315 / 450] loss: 1.855\n",
            "[Batch 360 / 450] loss: 1.848\n",
            "[Batch 405 / 450] loss: 1.841\n",
            "[Batch 450 / 450] loss: 1.844\n",
            "(Epoch 74 / 100) train acc: 60.78%; val_acc: 50.62%\n",
            "[Batch 45 / 450] loss: 1.845\n",
            "[Batch 90 / 450] loss: 1.848\n",
            "[Batch 135 / 450] loss: 1.853\n",
            "[Batch 180 / 450] loss: 1.846\n",
            "[Batch 225 / 450] loss: 1.849\n",
            "[Batch 270 / 450] loss: 1.860\n",
            "[Batch 315 / 450] loss: 1.853\n",
            "[Batch 360 / 450] loss: 1.833\n",
            "[Batch 405 / 450] loss: 1.858\n",
            "[Batch 450 / 450] loss: 1.861\n",
            "(Epoch 75 / 100) train acc: 60.81%; val_acc: 50.76%\n",
            "[Batch 45 / 450] loss: 1.856\n",
            "[Batch 90 / 450] loss: 1.851\n",
            "[Batch 135 / 450] loss: 1.852\n",
            "[Batch 180 / 450] loss: 1.836\n",
            "[Batch 225 / 450] loss: 1.855\n",
            "[Batch 270 / 450] loss: 1.849\n",
            "[Batch 315 / 450] loss: 1.851\n",
            "[Batch 360 / 450] loss: 1.839\n",
            "[Batch 405 / 450] loss: 1.851\n",
            "[Batch 450 / 450] loss: 1.849\n",
            "(Epoch 76 / 100) train acc: 61.10%; val_acc: 51.14%\n",
            "[Batch 45 / 450] loss: 1.847\n",
            "[Batch 90 / 450] loss: 1.847\n",
            "[Batch 135 / 450] loss: 1.858\n",
            "[Batch 180 / 450] loss: 1.851\n",
            "[Batch 225 / 450] loss: 1.843\n",
            "[Batch 270 / 450] loss: 1.856\n",
            "[Batch 315 / 450] loss: 1.851\n",
            "[Batch 360 / 450] loss: 1.850\n",
            "[Batch 405 / 450] loss: 1.840\n",
            "[Batch 450 / 450] loss: 1.849\n",
            "(Epoch 77 / 100) train acc: 61.02%; val_acc: 50.24%\n",
            "[Batch 45 / 450] loss: 1.862\n",
            "[Batch 90 / 450] loss: 1.850\n",
            "[Batch 135 / 450] loss: 1.848\n",
            "[Batch 180 / 450] loss: 1.846\n",
            "[Batch 225 / 450] loss: 1.852\n",
            "[Batch 270 / 450] loss: 1.840\n",
            "[Batch 315 / 450] loss: 1.849\n",
            "[Batch 360 / 450] loss: 1.853\n",
            "[Batch 405 / 450] loss: 1.847\n",
            "[Batch 450 / 450] loss: 1.838\n",
            "(Epoch 78 / 100) train acc: 61.09%; val_acc: 51.74%\n",
            "[Batch 45 / 450] loss: 1.851\n",
            "[Batch 90 / 450] loss: 1.840\n",
            "[Batch 135 / 450] loss: 1.847\n",
            "[Batch 180 / 450] loss: 1.857\n",
            "[Batch 225 / 450] loss: 1.852\n",
            "[Batch 270 / 450] loss: 1.853\n",
            "[Batch 315 / 450] loss: 1.845\n",
            "[Batch 360 / 450] loss: 1.850\n",
            "[Batch 405 / 450] loss: 1.840\n",
            "[Batch 450 / 450] loss: 1.840\n",
            "(Epoch 79 / 100) train acc: 61.23%; val_acc: 50.86%\n",
            "[Batch 45 / 450] loss: 1.846\n",
            "[Batch 90 / 450] loss: 1.839\n",
            "[Batch 135 / 450] loss: 1.846\n",
            "[Batch 180 / 450] loss: 1.842\n",
            "[Batch 225 / 450] loss: 1.847\n",
            "[Batch 270 / 450] loss: 1.850\n",
            "[Batch 315 / 450] loss: 1.845\n",
            "[Batch 360 / 450] loss: 1.856\n",
            "[Batch 405 / 450] loss: 1.842\n",
            "[Batch 450 / 450] loss: 1.845\n",
            "(Epoch 80 / 100) train acc: 61.38%; val_acc: 50.28%\n",
            "[Batch 45 / 450] loss: 1.851\n",
            "[Batch 90 / 450] loss: 1.838\n",
            "[Batch 135 / 450] loss: 1.842\n",
            "[Batch 180 / 450] loss: 1.830\n",
            "[Batch 225 / 450] loss: 1.841\n",
            "[Batch 270 / 450] loss: 1.846\n",
            "[Batch 315 / 450] loss: 1.849\n",
            "[Batch 360 / 450] loss: 1.838\n",
            "[Batch 405 / 450] loss: 1.860\n",
            "[Batch 450 / 450] loss: 1.844\n",
            "(Epoch 81 / 100) train acc: 61.58%; val_acc: 50.76%\n",
            "[Batch 45 / 450] loss: 1.837\n",
            "[Batch 90 / 450] loss: 1.851\n",
            "[Batch 135 / 450] loss: 1.840\n",
            "[Batch 180 / 450] loss: 1.843\n",
            "[Batch 225 / 450] loss: 1.838\n",
            "[Batch 270 / 450] loss: 1.852\n",
            "[Batch 315 / 450] loss: 1.842\n",
            "[Batch 360 / 450] loss: 1.841\n",
            "[Batch 405 / 450] loss: 1.838\n",
            "[Batch 450 / 450] loss: 1.847\n",
            "(Epoch 82 / 100) train acc: 61.76%; val_acc: 49.84%\n",
            "[Batch 45 / 450] loss: 1.837\n",
            "[Batch 90 / 450] loss: 1.848\n",
            "[Batch 135 / 450] loss: 1.833\n",
            "[Batch 180 / 450] loss: 1.837\n",
            "[Batch 225 / 450] loss: 1.834\n",
            "[Batch 270 / 450] loss: 1.839\n",
            "[Batch 315 / 450] loss: 1.847\n",
            "[Batch 360 / 450] loss: 1.846\n",
            "[Batch 405 / 450] loss: 1.841\n",
            "[Batch 450 / 450] loss: 1.843\n",
            "(Epoch 83 / 100) train acc: 61.84%; val_acc: 50.68%\n",
            "[Batch 45 / 450] loss: 1.842\n",
            "[Batch 90 / 450] loss: 1.845\n",
            "[Batch 135 / 450] loss: 1.854\n",
            "[Batch 180 / 450] loss: 1.847\n",
            "[Batch 225 / 450] loss: 1.847\n",
            "[Batch 270 / 450] loss: 1.840\n",
            "[Batch 315 / 450] loss: 1.835\n",
            "[Batch 360 / 450] loss: 1.851\n",
            "[Batch 405 / 450] loss: 1.837\n",
            "[Batch 450 / 450] loss: 1.842\n",
            "(Epoch 84 / 100) train acc: 61.47%; val_acc: 50.62%\n",
            "[Batch 45 / 450] loss: 1.845\n",
            "[Batch 90 / 450] loss: 1.842\n",
            "[Batch 135 / 450] loss: 1.853\n",
            "[Batch 180 / 450] loss: 1.843\n",
            "[Batch 225 / 450] loss: 1.837\n",
            "[Batch 270 / 450] loss: 1.835\n",
            "[Batch 315 / 450] loss: 1.841\n",
            "[Batch 360 / 450] loss: 1.831\n",
            "[Batch 405 / 450] loss: 1.838\n",
            "[Batch 450 / 450] loss: 1.848\n",
            "(Epoch 85 / 100) train acc: 61.82%; val_acc: 50.56%\n",
            "[Batch 45 / 450] loss: 1.836\n",
            "[Batch 90 / 450] loss: 1.846\n",
            "[Batch 135 / 450] loss: 1.844\n",
            "[Batch 180 / 450] loss: 1.841\n",
            "[Batch 225 / 450] loss: 1.832\n",
            "[Batch 270 / 450] loss: 1.834\n",
            "[Batch 315 / 450] loss: 1.844\n",
            "[Batch 360 / 450] loss: 1.838\n",
            "[Batch 405 / 450] loss: 1.845\n",
            "[Batch 450 / 450] loss: 1.835\n",
            "(Epoch 86 / 100) train acc: 62.06%; val_acc: 51.28%\n",
            "[Batch 45 / 450] loss: 1.837\n",
            "[Batch 90 / 450] loss: 1.825\n",
            "[Batch 135 / 450] loss: 1.834\n",
            "[Batch 180 / 450] loss: 1.856\n",
            "[Batch 225 / 450] loss: 1.845\n",
            "[Batch 270 / 450] loss: 1.849\n",
            "[Batch 315 / 450] loss: 1.843\n",
            "[Batch 360 / 450] loss: 1.854\n",
            "[Batch 405 / 450] loss: 1.833\n",
            "[Batch 450 / 450] loss: 1.848\n",
            "(Epoch 87 / 100) train acc: 61.71%; val_acc: 52.08%\n",
            "[Batch 45 / 450] loss: 1.837\n",
            "[Batch 90 / 450] loss: 1.854\n",
            "[Batch 135 / 450] loss: 1.839\n",
            "[Batch 180 / 450] loss: 1.833\n",
            "[Batch 225 / 450] loss: 1.837\n",
            "[Batch 270 / 450] loss: 1.838\n",
            "[Batch 315 / 450] loss: 1.842\n",
            "[Batch 360 / 450] loss: 1.841\n",
            "[Batch 405 / 450] loss: 1.847\n",
            "[Batch 450 / 450] loss: 1.832\n",
            "(Epoch 88 / 100) train acc: 61.97%; val_acc: 51.18%\n",
            "[Batch 45 / 450] loss: 1.825\n",
            "[Batch 90 / 450] loss: 1.826\n",
            "[Batch 135 / 450] loss: 1.836\n",
            "[Batch 180 / 450] loss: 1.832\n",
            "[Batch 225 / 450] loss: 1.845\n",
            "[Batch 270 / 450] loss: 1.832\n",
            "[Batch 315 / 450] loss: 1.836\n",
            "[Batch 360 / 450] loss: 1.851\n",
            "[Batch 405 / 450] loss: 1.847\n",
            "[Batch 450 / 450] loss: 1.836\n",
            "(Epoch 89 / 100) train acc: 62.36%; val_acc: 50.80%\n",
            "[Batch 45 / 450] loss: 1.842\n",
            "[Batch 90 / 450] loss: 1.839\n",
            "[Batch 135 / 450] loss: 1.840\n",
            "[Batch 180 / 450] loss: 1.835\n",
            "[Batch 225 / 450] loss: 1.841\n",
            "[Batch 270 / 450] loss: 1.835\n",
            "[Batch 315 / 450] loss: 1.832\n",
            "[Batch 360 / 450] loss: 1.839\n",
            "[Batch 405 / 450] loss: 1.837\n",
            "[Batch 450 / 450] loss: 1.832\n",
            "(Epoch 90 / 100) train acc: 62.19%; val_acc: 50.90%\n",
            "[Batch 45 / 450] loss: 1.826\n",
            "[Batch 90 / 450] loss: 1.838\n",
            "[Batch 135 / 450] loss: 1.828\n",
            "[Batch 180 / 450] loss: 1.838\n",
            "[Batch 225 / 450] loss: 1.844\n",
            "[Batch 270 / 450] loss: 1.848\n",
            "[Batch 315 / 450] loss: 1.838\n",
            "[Batch 360 / 450] loss: 1.833\n",
            "[Batch 405 / 450] loss: 1.843\n",
            "[Batch 450 / 450] loss: 1.843\n",
            "(Epoch 91 / 100) train acc: 62.18%; val_acc: 50.74%\n",
            "[Batch 45 / 450] loss: 1.841\n",
            "[Batch 90 / 450] loss: 1.838\n",
            "[Batch 135 / 450] loss: 1.832\n",
            "[Batch 180 / 450] loss: 1.835\n",
            "[Batch 225 / 450] loss: 1.837\n",
            "[Batch 270 / 450] loss: 1.842\n",
            "[Batch 315 / 450] loss: 1.829\n",
            "[Batch 360 / 450] loss: 1.846\n",
            "[Batch 405 / 450] loss: 1.832\n",
            "[Batch 450 / 450] loss: 1.833\n",
            "(Epoch 92 / 100) train acc: 62.36%; val_acc: 51.58%\n",
            "[Batch 45 / 450] loss: 1.830\n",
            "[Batch 90 / 450] loss: 1.822\n",
            "[Batch 135 / 450] loss: 1.833\n",
            "[Batch 180 / 450] loss: 1.836\n",
            "[Batch 225 / 450] loss: 1.837\n",
            "[Batch 270 / 450] loss: 1.837\n",
            "[Batch 315 / 450] loss: 1.839\n",
            "[Batch 360 / 450] loss: 1.846\n",
            "[Batch 405 / 450] loss: 1.838\n",
            "[Batch 450 / 450] loss: 1.838\n",
            "(Epoch 93 / 100) train acc: 62.34%; val_acc: 51.56%\n",
            "[Batch 45 / 450] loss: 1.825\n",
            "[Batch 90 / 450] loss: 1.831\n",
            "[Batch 135 / 450] loss: 1.822\n",
            "[Batch 180 / 450] loss: 1.836\n",
            "[Batch 225 / 450] loss: 1.837\n",
            "[Batch 270 / 450] loss: 1.833\n",
            "[Batch 315 / 450] loss: 1.839\n",
            "[Batch 360 / 450] loss: 1.840\n",
            "[Batch 405 / 450] loss: 1.841\n",
            "[Batch 450 / 450] loss: 1.835\n",
            "(Epoch 94 / 100) train acc: 62.56%; val_acc: 51.42%\n",
            "[Batch 45 / 450] loss: 1.830\n",
            "[Batch 90 / 450] loss: 1.835\n",
            "[Batch 135 / 450] loss: 1.822\n",
            "[Batch 180 / 450] loss: 1.835\n",
            "[Batch 225 / 450] loss: 1.828\n",
            "[Batch 270 / 450] loss: 1.839\n",
            "[Batch 315 / 450] loss: 1.836\n",
            "[Batch 360 / 450] loss: 1.837\n",
            "[Batch 405 / 450] loss: 1.828\n",
            "[Batch 450 / 450] loss: 1.835\n",
            "(Epoch 95 / 100) train acc: 62.74%; val_acc: 51.20%\n",
            "[Batch 45 / 450] loss: 1.821\n",
            "[Batch 90 / 450] loss: 1.828\n",
            "[Batch 135 / 450] loss: 1.840\n",
            "[Batch 180 / 450] loss: 1.838\n",
            "[Batch 225 / 450] loss: 1.829\n",
            "[Batch 270 / 450] loss: 1.831\n",
            "[Batch 315 / 450] loss: 1.832\n",
            "[Batch 360 / 450] loss: 1.838\n",
            "[Batch 405 / 450] loss: 1.835\n",
            "[Batch 450 / 450] loss: 1.841\n",
            "(Epoch 96 / 100) train acc: 62.60%; val_acc: 51.20%\n",
            "[Batch 45 / 450] loss: 1.822\n",
            "[Batch 90 / 450] loss: 1.829\n",
            "[Batch 135 / 450] loss: 1.833\n",
            "[Batch 180 / 450] loss: 1.831\n",
            "[Batch 225 / 450] loss: 1.843\n",
            "[Batch 270 / 450] loss: 1.834\n",
            "[Batch 315 / 450] loss: 1.830\n",
            "[Batch 360 / 450] loss: 1.825\n",
            "[Batch 405 / 450] loss: 1.847\n",
            "[Batch 450 / 450] loss: 1.849\n",
            "(Epoch 97 / 100) train acc: 62.48%; val_acc: 51.24%\n",
            "[Batch 45 / 450] loss: 1.829\n",
            "[Batch 90 / 450] loss: 1.839\n",
            "[Batch 135 / 450] loss: 1.844\n",
            "[Batch 180 / 450] loss: 1.834\n",
            "[Batch 225 / 450] loss: 1.815\n",
            "[Batch 270 / 450] loss: 1.833\n",
            "[Batch 315 / 450] loss: 1.827\n",
            "[Batch 360 / 450] loss: 1.840\n",
            "[Batch 405 / 450] loss: 1.834\n",
            "[Batch 450 / 450] loss: 1.833\n",
            "(Epoch 98 / 100) train acc: 62.69%; val_acc: 50.56%\n",
            "[Batch 45 / 450] loss: 1.824\n",
            "[Batch 90 / 450] loss: 1.843\n",
            "[Batch 135 / 450] loss: 1.825\n",
            "[Batch 180 / 450] loss: 1.847\n",
            "[Batch 225 / 450] loss: 1.831\n",
            "[Batch 270 / 450] loss: 1.825\n",
            "[Batch 315 / 450] loss: 1.837\n",
            "[Batch 360 / 450] loss: 1.816\n",
            "[Batch 405 / 450] loss: 1.832\n",
            "[Batch 450 / 450] loss: 1.833\n",
            "(Epoch 99 / 100) train acc: 62.86%; val_acc: 50.56%\n",
            "[Batch 45 / 450] loss: 1.828\n",
            "[Batch 90 / 450] loss: 1.834\n",
            "[Batch 135 / 450] loss: 1.828\n",
            "[Batch 180 / 450] loss: 1.818\n",
            "[Batch 225 / 450] loss: 1.832\n",
            "[Batch 270 / 450] loss: 1.822\n",
            "[Batch 315 / 450] loss: 1.845\n",
            "[Batch 360 / 450] loss: 1.838\n",
            "[Batch 405 / 450] loss: 1.829\n",
            "[Batch 450 / 450] loss: 1.828\n",
            "(Epoch 100 / 100) train acc: 62.94%; val_acc: 50.98%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLES_37SM6_N"
      },
      "source": [
        "#### 3.6. Test: \n",
        "Run the following cell and test your network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw4zW0GPM6cR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1b18381-71ee-4c9c-c6a4-462a9173b044"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "test_acc = correct / total\n",
        "print('Accuracy of the network on the test images: %2f %%' % (100 * test_acc ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 50.430000 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrnQkpyENTrR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c543acf0-aed5-4684-fdc5-d3c36b81e93b"
      },
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 55 %\n",
            "Accuracy of   car : 52 %\n",
            "Accuracy of  bird : 29 %\n",
            "Accuracy of   cat : 38 %\n",
            "Accuracy of  deer : 33 %\n",
            "Accuracy of   dog : 28 %\n",
            "Accuracy of  frog : 57 %\n",
            "Accuracy of horse : 66 %\n",
            "Accuracy of  ship : 62 %\n",
            "Accuracy of truck : 68 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvB7rqNoWKNV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0b61f8d-5519-4eae-be4f-f0cd8aad844d"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JWZmfK7KqZ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9784249-4239-469f-eb30-d724a67a6432"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNmHRKPxLfa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "fe3a609f-1fa1-496d-d75b-5078f1076af9"
      },
      "source": [
        "ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mbin\u001b[0m/                                       \u001b[01;34metc\u001b[0m/    \u001b[01;34mopt\u001b[0m/    \u001b[01;34msys\u001b[0m/\n",
            "\u001b[01;34mboot\u001b[0m/                                      \u001b[01;34mhome\u001b[0m/   \u001b[01;34mproc\u001b[0m/   \u001b[01;34mtensorflow-1.15.2\u001b[0m/\n",
            "\u001b[01;34mcontent\u001b[0m/                                   \u001b[01;34mlib\u001b[0m/    \u001b[01;34mroot\u001b[0m/   \u001b[30;42mtmp\u001b[0m/\n",
            "\u001b[01;34mdatalab\u001b[0m/                                   \u001b[01;34mlib32\u001b[0m/  \u001b[01;34mrun\u001b[0m/    \u001b[01;34mtools\u001b[0m/\n",
            "\u001b[01;34mdev\u001b[0m/                                       \u001b[01;34mlib64\u001b[0m/  \u001b[01;34msbin\u001b[0m/   \u001b[01;34musr\u001b[0m/\n",
            "dlib-19.18.0-cp27-cp27mu-linux_x86_64.whl  \u001b[01;34mmedia\u001b[0m/  \u001b[01;34msrv\u001b[0m/    \u001b[01;34mvar\u001b[0m/\n",
            "dlib-19.18.0-cp36-cp36m-linux_x86_64.whl   \u001b[01;34mmnt\u001b[0m/    \u001b[01;34mswift\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyDE6gUZLiTL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c5d0f3a-2bdc-477e-9774-3e295a50484c"
      },
      "source": [
        "cd content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "givrPHEUL3xV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9fe219f-96f8-442c-bcbb-7a94f463bcae"
      },
      "source": [
        "ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AQsE-g9L4zm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dd2dd6d1-00a3-4a76-bbf2-4dffc5346136"
      },
      "source": [
        "cd gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'gdrive'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLPpl3h-MIeR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ae624ea2-d750-4f09-d148-813ae4ea335a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UDdSYMJMo-0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f46872f-574d-4759-eb64-7178586c45fd"
      },
      "source": [
        "ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apTqfVseNSDq"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5rfUEP5OBam",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "305ee578-a17d-4acb-c453-d970d1bcc093"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}