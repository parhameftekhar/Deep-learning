{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE40719: Deep Learning\n",
    "## HW3  \n",
    "\n",
    "\n",
    "#### Name:\n",
    "#### Student No.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. NumPy Based CNN block (15+5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deadline:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from CE40719.module import *\n",
    "from CE40719.batch_norm import *\n",
    "from CE40719.cnn_batch_norm import *\n",
    "from CE40719.convolution import *\n",
    "from CE40719.group_batch_norm import *\n",
    "from CE40719.max_pool import *\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Outline of the Assignment\n",
    "\n",
    "You will be implementing the building blocks of a convolutional neural network\n",
    "    \n",
    "This notebook will ask you to implement these functions from scratch in **`Numpy`**.\n",
    " \n",
    "1. **`convolution `** \n",
    "\n",
    "2. **`Max pooling`** \n",
    "\n",
    "3. **`CNN Batch Normalization`**: Batch Normalization layer in cnn\n",
    "\n",
    "4. **`Group Normalization`**: layer which normalize over the group of feature per-datapoint(E.g. group of pixel per-image on first layer of network)\n",
    "\n",
    "\n",
    "In this problem you have to implement `forward()` and `backward()` functions of the modules in the above list. Then your implemented codes will be tested here in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution (7 Points)\n",
    "Open file `convolution.py`. There you see a class for convolutione layer used in CNN networks.<br/> \n",
    "\n",
    "Implement `forward()` and `backward()` functions corresponding to forward and backward pass of a concolution layer.<br/>\n",
    "We have implemented `__init__()` constructor for you. You can see `W` with shape of $(F, C, f_h, f_w)$, `b` with shape of $(F,)$, `dW`, `db`, `stride` number of pixels between adjacent receptive fields in the horizontal and vertical directions and `pad` number of pixels that will be used to zeropad the input variables in this class.<br/>\n",
    "The input of forward pass is X with shape of $(N, C, i_h, i_w)$, The return value of forward pass is o the shape is $(N, F, O_h, O_w)$ where \n",
    "\n",
    "$$\\text{stride} : int $$\n",
    "\n",
    "$$\\text{pad} : int$$\n",
    "\n",
    "$$O_w =\\lfloor \\frac{i_w - f_w + 2*pad}{stride} \\rfloor + 1$$\n",
    "\n",
    "$$O_h = \\lfloor\\frac{i_h - f_h + 2*pad}{stride}\\rfloor + 1$$\n",
    "$$O(b,f, i ,j)=\\sum_{r=0}^{C-1}\\sum_{k=0}^{f_h-1}\\sum_{l=0}^{f_w-1} W(f,r,k,l) X(b,r,s_h *i +k, s_w  *j +l)+b(f)$$.<br/>\n",
    "In the backward pass you get $\\large{\\frac{\\partial{loss}}{\\partial{y}}}$ as `dout`; You have to compute $\\large{\\frac{\\partial{loss}}{\\partial{X}}}$, $\\large{\\frac{\\partial{loss}}{\\partial{W}}}$ and $\\large{\\frac{\\partial{loss}}{\\partial{b}}}$ and save them in `dx`, `dW`, and `db`. The output of the `backward()` function will be `dx`.<br/>\n",
    "This is the formula for computing a $\\frac{\\partial L}{\\partial X}$ for a single $X(b^\\prime,c^\\prime,k^\\prime ,l^\\prime )$ that $X$ is 4-D array as a input in convolution operation with shape $(N,C,i_h,i_w)$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X(b^\\prime,c^\\prime,k^\\prime ,l^\\prime )} = \\sum_{f=0}^{F-1}\\left (\\sum_{i=0}^{O_h-1}\\sum_{j=0}^{O_w-1} \\frac{\\partial L}{\\partial O(b^\\prime,f,i,j)} \\frac{\\partial O(b^\\prime,f,i,j)}{\\partial X(b^\\prime,c^\\prime,k^\\prime ,l^\\prime )}\\right ) = \\sum_{f=0}^{F-1}\\left (\\sum_{i=0}^{O_h-1}\\sum_{j=0}^{O_w-1} \\frac{\\partial L}{\\partial O(b^\\prime,f,i,j)} W(f,c^\\prime,k^\\prime - s_h*i, l^\\prime - s_w*j) \\right )$$<br/>\n",
    "\n",
    " This is the formula for computing a $\\frac{\\partial L}{\\partial W}$ for a single $W(f^\\prime,c^\\prime,k^\\prime ,l^\\prime )$ that $W$ is 4-D array as a filter in convolution operation with shape $(F,C,f_h,f_w)$\n",
    " \n",
    " $$\\frac{\\partial L}{\\partial W(f^\\prime,c^\\prime,k^\\prime ,l^\\prime )} = \\sum_{b=0}^{N-1}\\left (\\sum_{i=0}^{O_h-1}\\sum_{j=0}^{O_w-1} \\frac{\\partial L}{\\partial O(b,f^\\prime,i,j)} \\frac{\\partial O(i,j)}{\\partial W(f^\\prime,c^\\prime,k^\\prime ,l^\\prime )}\\right ) = \\sum_{b=0}^{N-1}\\left (\\sum_{i=0}^{O_w-1}\\sum_{j=0}^{O_h-1} \\frac{\\partial L}{\\partial O(b,f^\\prime,i,j)}  X(b,c^\\prime, s_h*i +k^\\prime, s_w*j +l^\\prime) \\right )$$<br/>\n",
    " the formula for $\\large{\\frac{\\partial{loss}}{\\partial{b}}}$ is left for you to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error forward pass: 5.014143567861652e-08\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bb3b86046d15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.07933521\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.62650629\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.35726596\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.17660094\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                         [[ 0.27806844,  2.30231871],[-1.07156607, 0.22142858]]]])\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Relative error dx:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcorrect_dx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m correct_dW =np.array([[[[0.16162545,  0.44372442],[ 0.5131281 , 0.41785749]],\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "#                         Convolution Test                                #\n",
    "###########################################################################\n",
    "np.random.seed(40959)\n",
    "x = np.random.randn(2, 3, 4, 4)\n",
    "w = np.random.randn(3, 3, 4, 4)\n",
    "b = np.random.randn(3,)\n",
    "conv = Convolution('test', (3, 3, 4, 4), stride = 2, pad = 1)\n",
    "conv.W = w\n",
    "conv.b = b\n",
    "output = conv.forward(x)\n",
    "correct_output = np.array([[[[2.90745973, -1.93569447],\n",
    "                           [10.18479326, 0.95405206 ]],\n",
    "                          [[ 2.67941204,  -1.06629218],\n",
    "                           [ -2.6427213,  -3.12561258]],\n",
    "                          [[ -5.32408346,  1.12473438],\n",
    "                           [ 4.16451343,  -5.04230883]]],\n",
    "                         [[[0.18517581, 10.22485798],\n",
    "                           [-3.51174763, 1.9202936]],\n",
    "                          [[ -2.56595929,  -3.40545467],\n",
    "                           [ 0.33082083,  4.34434771]],\n",
    "                          [[ -3.54337648,  2.44988087],\n",
    "                           [ -3.6369818,  1.96857427]]]])\n",
    "\n",
    "print('Relative error forward pass:', np.linalg.norm(output - correct_output))\n",
    "\n",
    "\n",
    "conv = Convolution('test', (2, 3, 2, 2), stride = 2, pad = 1)\n",
    "x = np.random.randn(2, 3, 2, 2)\n",
    "W = np.random.randn(2, 3, 2, 2)\n",
    "b = np.random.randn(2,)\n",
    "\n",
    "conv.W = W\n",
    "conv.b = b\n",
    "dout = np.random.randn(2, 2, 2, 2)\n",
    "out = conv.forward(x)\n",
    "dx= conv.backward(dout)\n",
    "\n",
    "\n",
    "correct_dx = np.array([[[[-0.03022149, -0.93652977],[-0.05179407, 1.62558139]],\n",
    "                        [[ 1.62356625,  3.17432728],[1.37585703,  0.21801443]],\n",
    "                        [[ -1.14110006,  -3.2751212],[0.98650008,   0.78396852 ]]],\n",
    "                       [[[0.48556001,  1.24240355],[ 0.1635526,  0.97860699]],\n",
    "                        [[2.07933521, -1.62650629],[ -0.35726596, 0.17660094]],\n",
    "                        [[ 0.27806844,  2.30231871],[-1.07156607, 0.22142858]]]])\n",
    "print('Relative error dx:', np.linalg.norm(dx - correct_dx))\n",
    "    \n",
    "correct_dW =np.array([[[[0.16162545,  0.44372442],[ 0.5131281 , 0.41785749]],\n",
    "                       [[-0.4409529, -0.31301584],[-0.18734267,  0.06869406]],\n",
    "                       [[ -0.53426167, -0.94735183],[ 0.9614619, 0.36417281]]],\n",
    "                      [[[ -0.40656537, -0.1906337  ],[1.38892306, -0.59866861]],\n",
    "                       [[ 0.81392044, 0.36665929 ],[0.78840142, 2.80736748]],\n",
    "                       [[1.58139656, -0.81670389],[ -1.11075549,  -1.7656368]]]])\n",
    "print('Relative error dw:', np.linalg.norm(conv.dW - correct_dW))\n",
    "    \n",
    "correct_db =np.array([0.82375129, 2.84032899])\n",
    "print('Relative error db:', np.linalg.norm(conv.db - correct_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output: **\n",
    "<table>    \n",
    "    <tr>\n",
    "        <td>\n",
    "            **Relative error forward pass**\n",
    "        </td>\n",
    "        <td>\n",
    "            5.0141436175352257e-08\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Relative error dx**\n",
    "        </td>\n",
    "        <td>\n",
    "            1.3685826529800791e-08\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Relative error dw**\n",
    "        </td>\n",
    "        <td>\n",
    "            1.3702887296620126e-08\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Relative error db**\n",
    "        </td>\n",
    "        <td>\n",
    "            2.2920873066590985e-09\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max-Pooling (5 Points)\n",
    "The pooling layer reduces the height and width of the input. It helps reduce computation<br/>\n",
    "Max-pooling layer slides an ($f, f$) window with stride $s$ over the input and stores the max value of the window in the output.\n",
    "Open file `max_pool.py`. There you see a class for pooling layer.<br/> \n",
    "\n",
    "We have implemented `__init__()` constructor for you. Similar to `Convolution` class, there are pool hight,  pool width and stride variables in this class.\n",
    "\n",
    "Implement `forward()` and `backward()` functions corresponding to forward and backward pass of pooling layer. Even though a pooling layer has no parameters for backprop to update, you still need to backpropagation the gradient through the pooling layer in order to compute gradients for layers that came before the pooling layer.<br/>\n",
    "in `forward()` function X is input and shape of X is $(N, C, i_h, i_w)$  and output is shape $(N, C, O_h, O_w)$ that :\n",
    "\n",
    " $$O_h =\\lfloor\\frac{i_h - f }{s}\\rfloor + 1$$\n",
    " $$O_w =\\lfloor\\frac{i_w - f }{s}\\rfloor + 1$$\n",
    "\n",
    " No padding is necessary here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error forward pass: 1.4161709469389779e-08\n",
      "Relative error dx: 8.555596854220575e-09\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "#                            Max pool test                                #\n",
    "###########################################################################\n",
    "np.random.seed(40959)\n",
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "pool = MaxPool('test',height = 2,width = 2, stride = 2)\n",
    "output = pool.forward(x)\n",
    "\n",
    "correct_output = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "print('Relative error forward pass:', np.linalg.norm(output - correct_output))\n",
    "\n",
    "x = np.random.randn(3, 2, 2, 2)\n",
    "dout = np.random.randn(3, 2, 1, 1)\n",
    "pool = MaxPool('test',height = 2, width = 2, stride =2)\n",
    "out = pool.forward(x)\n",
    "dx = pool.backward(dout)\n",
    "correct_dx = np.array([[[[0., 1.21236066],[0., 0.]],\n",
    "                        [[0.45107133, 0.],[0., 0.]]],\n",
    "                       [[[0., 0.],[-0.86463156,0.]],\n",
    "                        [[0., 0.],[0., -0.39180953]]],\n",
    "                       [[[0.93694169, 0.],[0., 0.]],\n",
    "                        [[0., 0.],[-0.08002411, 0.]]]])\n",
    "print('Relative error dx:', np.linalg.norm(dx - correct_dx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output: **\n",
    "<table>    \n",
    "    <tr>\n",
    "        <td>\n",
    "            **Relative error forward pass**\n",
    "        </td>\n",
    "        <td>\n",
    "            4.1666665157267834e-08\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Relative error dx**\n",
    "        </td>\n",
    "        <td>\n",
    "            8.555596854220575e-09\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN batch normalization(3 Points)\n",
    "Normally batch-normalization accepts inputs of shape `(N, D)` and produces outputs of shape `(N, D)`, where we normalize across the minibatch dimension `N`. For data coming from convolutional layers, batch normalization needs to accept inputs of shape $(F, C, f_h, f_w)$ and produce outputs of shape $(F, C, f_h, f_w)$ where the `N` dimension gives the minibatch size and the $(f_h, f_w)$ dimensions give the spatial size of the feature map.\n",
    "\n",
    "If the feature map was produced using convolutions, then we expect every feature channel's statistics e.g. mean, variance to be relatively consistent both between different images, and different locations within the same image -- after all, every feature channel is produced by the same convolutional filter! Therefore spatial batch normalization computes a mean and variance for each of the `C` feature channels by computing statistics over the minibatch dimension `N` as well the spatial dimensions  $f_h$ and  $f_h$.\n",
    "\n",
    "Open file `cnn_batch_norm.py`. There you see a class for CNN Batch Normalization layer.<br/> \n",
    "\n",
    "Implement `forward()` and `backward()` functions corresponding to forward and backward pass of cnn Batch Normalization layer.<br/>\n",
    " \n",
    "We have implemented `__init__()` constructor for you. Similar to `Convolution` class, there are `gamma`, `beta`, `dgamma` and `dbeta` variables in this class. also there is one auxiliary variable `batchnorm` from previous assignment witch will help you to implement cnn batchnormalization easily.\n",
    "Test your implementations with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#            Batch-Normalization forward pass Test                        #\n",
    "###########################################################################\n",
    "np.random.seed(40959)\n",
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after spatial batch normalization\n",
    "\n",
    "N, C, H, W = 2, 3, 4, 5\n",
    "x = 4 * np.random.randn(N, C, H, W) + 10\n",
    "\n",
    "print('Before spatial batch normalization:')\n",
    "print('  Shape: ', x.shape)\n",
    "print('  Means: ', x.mean(axis=(0, 2, 3)))\n",
    "print('  Stds: ', x.std(axis=(0, 2, 3)))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "gamma, beta = np.ones(C), np.zeros(C)\n",
    "cnn_batchnorm = CnnBatchNorm('test',(N, C, H, W))\n",
    "cnn_batchnorm.batchnorm.gamma = gamma\n",
    "cnn_batchnorm.batchnorm.beta = beta\n",
    "out = cnn_batchnorm.forward(x)\n",
    "print('After spatial batch normalization:')\n",
    "print('  Shape: ', out.shape)\n",
    "print('  Means: ', out.mean(axis=(0, 2, 3)))\n",
    "print('  Stds: ', out.std(axis=(0, 2, 3)))\n",
    "\n",
    "# Means should be close to beta and stds close to gamma\n",
    "gamma, beta = np.asarray([3, 4, 5]), np.asarray([6, 7, 8])\n",
    "cnn_batchnorm.batchnorm.gamma = gamma\n",
    "cnn_batchnorm.batchnorm.beta = beta\n",
    "out= cnn_batchnorm.forward(x)\n",
    "print('After spatial batch normalization (nontrivial gamma, beta):')\n",
    "print('  Shape: ', out.shape)\n",
    "print('  Means: ', out.mean(axis=(0, 2, 3)))\n",
    "print('  Stds: ', out.std(axis=(0, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(40959)\n",
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "N, C, H, W = 10, 4, 11, 12\n",
    "cnn_batchnorm = CnnBatchNorm('test',(N, C, H, W))\n",
    "cnn_batchnorm.train()\n",
    "for t in range(50):\n",
    "  x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
    "  cnn_batchnorm.forward(x)\n",
    "cnn_batchnorm.test()\n",
    "x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
    "a_norm = cnn_batchnorm.forward(x)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print('After spatial batch normalization (test-time):')\n",
    "print('  means: ', a_norm.mean(axis=(0, 2, 3)))\n",
    "print('  stds: ', a_norm.std(axis=(0, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#            Batch-Normalization backward pass Test                       #\n",
    "###########################################################################\n",
    "np.random.seed(40959)\n",
    "N, C, H, W = 2, 3, 2, 2\n",
    "x = 5 * np.random.randn(N, C, H, W) + 12\n",
    "gamma = np.random.randn(C)\n",
    "beta = np.random.randn(C)\n",
    "dout = np.random.randn(N, C, H, W)\n",
    "cnn_batchnorm = CnnBatchNorm('Test',(N, C, H, W))\n",
    "cnn_batchnorm.batchnorm.gamma = gamma\n",
    "cnn_batchnorm.batchnorm.beta = beta\n",
    "cnn_batchnorm.train()\n",
    "_ = cnn_batchnorm.forward(x)\n",
    "dx = cnn_batchnorm.backward(dout)\n",
    "dgamma = cnn_batchnorm.dgamma\n",
    "dbeta = cnn_batchnorm.dbeta\n",
    "correct_dx = np.array([[[[0.00589789,  1.2557341 ],[-0.18515455, -0.3084614 ]],\n",
    "                        [[-0.04023214, -0.11912787],[-0.04556006, -0.00270806]],\n",
    "                        [[ 0.12266522, -0.07093585],[ 0.22957267,  0.17611092]]],\n",
    "                       [[[ 0.36047414, -0.01314037],[-0.62981818, -0.48553163]],\n",
    "                        [[ 0.18630326, -0.02134853],[-0.15169621,  0.19436962]],\n",
    "                        [[-0.00739465, -0.04518148],[-0.2105455,  -0.19429132]]]])\n",
    "print('Relative error dx:', np.linalg.norm(dx - correct_dx))\n",
    "correct_dgamma = np.array([ 1.51945006, -1.09337409,  0.8928227 ])\n",
    "print('Relative error dgamma:', np.linalg.norm(dgamma - correct_dgamma))\n",
    "correct_dbeta = np.array([-3.1690584,   3.01154949,  5.44132887])\n",
    "print('Relative error dbeta:', np.linalg.norm(dbeta - correct_dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output: **\n",
    "<table>    \n",
    "    <tr>\n",
    "        <td>\n",
    "            **Relative error dx**\n",
    "        </td>\n",
    "        <td>\n",
    "            1.2516125487116332e-08\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Relative error dgamma**\n",
    "        </td>\n",
    "        <td>\n",
    "            4.2623728399775465e-09\n",
    "        </td>\n",
    "     </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Relative error dbeta**\n",
    "        </td>\n",
    "        <td>\n",
    "            6.5396326216333006e-09\n",
    "        </td>\n",
    "     </tr>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Normalization (5 extra Points)\n",
    "Batch normalization has proved to be effective in making networks easier to train, but the dependency on batch size makes it less useful in complex networks which have a cap on the input batch size due to hardware limitations. \n",
    "\n",
    "Several alternatives to batch normalization have been proposed to mitigate this problem; one such technique is Layer Normalization [1]. Instead of normalizing over the batch, we normalize over the features. In other words, when using Layer Normalization, each feature vector corresponding to a single datapoint is normalized based on the sum of all terms within that feature vector.\n",
    "\n",
    "Layer Normalization mitigates the batch size limitations of Batch Normalization. However, as the authors of [2] observed, Layer Normalization does not perform as well as Batch Normalization when used with Convolutional Layers:\n",
    "\n",
    ">With fully connected layers, all the hidden units in a layer tend to make similar contributions to the final prediction, and re-centering and rescaling the summed inputs to a layer works well. However, the assumption of similar contributions is no longer true for convolutional neural networks. The large number of the hidden units whose\n",
    "receptive fields lie near the boundary of the image are rarely turned on and thus have very different\n",
    "statistics from the rest of the hidden units within the same layer.\n",
    "\n",
    "The authors of [2] propose an intermediary technique. In contrast to Layer Normalization, where you normalize over the entire feature per-datapoint, they suggest a consistent splitting of each per-datapoint feature into G groups, and a per-group per-datapoint normalization instead. \n",
    "\n",
    "![Comparison of normalization techniques discussed so far](notebook_images/normalization.png)\n",
    "<center>**Visual comparison of the normalization techniques discussed so far (image edited from [2])**</center>\n",
    "\n",
    "Even though an assumption of equal contribution is still being made within each group, the authors hypothesize that this is not as problematic, as innate grouping arises within features for visual recognition. One example they use to illustrate this is that many high-performance handcrafted features in traditional Computer Vision have terms that are explicitly grouped together. Take for example Histogram of Oriented Gradients [3]-- after computing histograms per spatially local block, each per-block histogram is normalized before being concatenated together to form the final feature vector.\n",
    "\n",
    "You will now implement Group Normalization. Note that this normalization technique that you are to implement in the following cells was introduced and published to ECCV  in 2018 .\n",
    "\n",
    "[1] [Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. \"Layer Normalization.\" stat 1050 (2016): 21.](https://arxiv.org/pdf/1607.06450.pdf)\n",
    "\n",
    "\n",
    "[2] [Wu, Yuxin, and Kaiming He. \"Group Normalization.\" arXiv preprint arXiv:1803.08494 (2018).](https://arxiv.org/abs/1803.08494)\n",
    "\n",
    "\n",
    "[3] [N. Dalal and B. Triggs. Histograms of oriented gradients for\n",
    "human detection. In Computer Vision and Pattern Recognition\n",
    "(CVPR), 2005.](https://ieeexplore.ieee.org/abstract/document/1467360/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group normalization\n",
    "\n",
    "Open file `group_batch_norm.py`. There you see a class for group Batch Normalization layer.<br/> \n",
    "\n",
    "Implement `forward()` and `backward()` functions corresponding to forward and backward pass of group Batch Normalization layer.<br/>\n",
    " \n",
    "We have implemented `__init__()` constructor for you. Similar to `Convolution` class, there are `G` integer number of groups to split into which should be a divisor of number of channel, `gamma`, `beta`, `dgamma` and `dbeta` variables in this class. \n",
    "Test your implementations with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#            Group-Normalization forward Test                             #\n",
    "###########################################################################\n",
    "np.random.seed(40959)\n",
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after spatial batch normalization\n",
    "\n",
    "N, C, H, W = 2, 6, 4, 5\n",
    "G = 2\n",
    "x = 4 * np.random.randn(N, C, H, W) + 10\n",
    "x_g = x.reshape((N*G,-1))\n",
    "print('Before spatial group normalization:')\n",
    "print('  Shape: ', x.shape)\n",
    "print('  Means: ', x_g.mean(axis=1))\n",
    "print('  Stds: ', x_g.std(axis=1))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "norm = GroupBatchNorm('test',(N, C, H, W), G=2)\n",
    "out = norm.forward(x)\n",
    "out_g = out.reshape((N*G,-1))\n",
    "print('After spatial group normalization:')\n",
    "print('  Shape: ', out.shape)\n",
    "print('  Means: ', out_g.mean(axis=1))\n",
    "print('  Stds: ', out_g.std(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#            Group-Normalization backward Test                            #\n",
    "###########################################################################\n",
    "np.random.seed(40959)\n",
    "N, C, H, W = 2, 4, 2, 2\n",
    "G = 2\n",
    "x = 5 * np.random.randn(N, C, H, W) + 12\n",
    "dout = np.random.randn(N, C, H, W)\n",
    "norm = GroupBatchNorm('test',(N, C, H, W), G)\n",
    "_ = norm.forward(x)\n",
    "dx = norm.backward(dout)\n",
    "dgamma = norm.dgamma\n",
    "dbeta = norm.dbeta\n",
    "correct_dx = np.array([[[[-0.34315175, -0.12381128],[ 0.03543989, -0.10811583]],\n",
    "                        [[ 0.23704048,  0.26681368],[-0.35225047,  0.38803527]],\n",
    "                        [[ 0.08007765, -0.00905627],[-0.05405272, -0.10643134]],\n",
    "                        [[-0.09270617, -0.26378193],[ 0.36204868,  0.08390209]]],\n",
    "                       [[[-0.15694482,  0.13000584],[-0.10158775, -0.07278944]],\n",
    "                        [[ 0.29256627,  0.00665728],[-0.08065789, -0.0172495 ]],\n",
    "                        [[-0.01647735,  0.09978986],[-0.11498191,  0.05169455]],\n",
    "                        [[ 0.09743741, -0.18717901],[ 0.12091729, -0.05120085]]]])\n",
    "print('Relative error dx:', np.linalg.norm(dx - correct_dx))\n",
    "correct_dgamma = np.array([[[[-3.28150076]],[[-5.58372413]],[[ 2.98735869]],[[ 3.02274058]]]])\n",
    "print('Relative error dgamma:', np.linalg.norm(dgamma - correct_dgamma))\n",
    "correct_dbeta = np.array([[[[ 0.28621793]],[[ 5.8519097 ]],[[-3.48103948]],[[-2.95280322]]]])\n",
    "print('Relative error dbeta:', np.linalg.norm(dbeta - correct_dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output: **\n",
    "<table>    \n",
    "    <tr>\n",
    "        <td>\n",
    "            **Relative error dx**\n",
    "        </td>\n",
    "        <td>\n",
    "            1.4961328597826837e-08\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Relative error dgamma**\n",
    "        </td>\n",
    "        <td>\n",
    "            5.694257120494842e-09\n",
    "        </td>\n",
    "     </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Relative error dbeta**\n",
    "        </td>\n",
    "        <td>\n",
    "            4.410581982727471e-09\n",
    "        </td>\n",
    "     </tr>    \n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
