# Deep-learning course

# Assignment 1

* This assignment starts with some numpy warm-up exercises followed by the implementation of linear multi-class support vector machine (SVM) with soft-margin.


# Assignment 2
* **Part1:** In this part, different blocks of multi-layer perceptron (MLP) e.g. dense layer, ReLU, BatchNormalization to name a few are implemented using NumPy.

* **Part2:** The goal of this part is to train a MLP implemented in Part1 to classify the CIFAR100 dataset.

* **Part3:** In this part a MLP model is trained using PyTorch to classify the CIFAR10 dataset. The goal of this part is to getting familiar with PyTorch main modules such as torch.nn, torch.optim, Dataset, and Dataloader.

# Assignment 3
* **Part1:** In this part, different blocks of a convolutional neural network (CNN) is implemented e.g. convolution, max pooling, CNN batch normalization.

* **Part2:** In this part, a CNN is trained using PyTorch modules to classify the fashion-mnist dataset.

* **Part3:** In this part, a simple residual network architecture is implemented using PyTorch modules.

# Assignment4:

* **Part1:** In this part, a language model is trained on [Wikitext](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) using a stack of LSTMs. To overcome the problem of overfiting, different regularization techniques are used e.g. activity regularization, temporal activation regularization.


* **Part2:** The goal of this part is to introducing the preprocessing concepts of tokenization, vocabulary in NLP as well as working with PyTorch modules for padding and packing sequences and adding embedding layers.


* **Part3:** The goal of this part is to implement a English-to-French machine translation with attention mechanism using encoder-decoder architecture.

